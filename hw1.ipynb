{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1: Keyword extraction\n",
    "Prepare dataset and gold standard, use methods for extraction, and evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install bs4\n",
    "# pip3 install pandas\n",
    "# pip3 install stanza\n",
    "# pip3 install nltk\n",
    "# pip3 install python-rake\n",
    "# pip3 install summa\n",
    "# pip3 install sentence-transformers\n",
    "# pip3 install numpy\n",
    "# pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import requests\n",
    "import statistics\n",
    "import time\n",
    "from typing import Callable\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import RAKE\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import stanza\n",
    "import summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_sw = set(nltk.corpus.stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Let`s get the articles from [habr.com](https://habr.com/ru/all/) that are related with NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_site(url: str, ua: str) -> BeautifulSoup:\n",
    "    '''\n",
    "    Parse web-site with BeautifulSoup.\n",
    "    '''\n",
    "\n",
    "    res = requests.get(url, headers={'User-Agent': ua})\n",
    "    page = res.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uas = [\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:91.0) Gecko/20100101 Firefox/91.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.164 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.43 (KHTML, like Gecko) Chrome/91.0.4472.164 Safari/537.36 OPR/77.0.4054.277'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parse site with links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_NLP_RESULTS = 'https://habr.com/ru/search/?q=NLP&target_type=posts&order=date'\n",
    "\n",
    "search_nlp_results = parse_site(SEARCH_NLP_RESULTS, random.choice(uas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list = search_nlp_results.find(\n",
    "    'div',\n",
    "    {\n",
    "        'data-test-id': 'articles-list',\n",
    "        'class': 'tm-articles-list'\n",
    "    },\n",
    "    )\n",
    "articles_boxes = articles_list.find_all(\n",
    "    'article'\n",
    ")\n",
    "print('Статей на первой странице:', len(articles_boxes))\n",
    "\n",
    "nlp_links = []\n",
    "for box in articles_boxes[:10]:\n",
    "    title = box.find(\n",
    "        'a',\n",
    "        {\n",
    "            'class': 'tm-article-snippet__title-link'\n",
    "        }\n",
    "    )\n",
    "    link = title.attrs['href']\n",
    "    nlp_links.append('https://habr.com' + link)\n",
    "\n",
    "nlp_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parse articles with 5 or more tags (the value obtained empirically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords in this articles are presented in the tags section. Users tag their articles with some words or n-grams by which we can search related articles on the resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_data = {\n",
    "    'link': [],\n",
    "    'original tags': [],\n",
    "    'text': []\n",
    "}\n",
    "\n",
    "for link in nlp_links:\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    article = parse_site(link, random.choice(uas))\n",
    "\n",
    "    tags_a = article.find_all(\n",
    "        'a',\n",
    "        {\n",
    "            'class': 'tm-tags-list__link'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if len(tags_a) >= 5:\n",
    "\n",
    "        articles_data['link'].append(link)\n",
    "\n",
    "        # tags\n",
    "        tags = []\n",
    "        for tag_a in tags_a:\n",
    "            tags.append(tag_a.text)\n",
    "        articles_data['original tags'].append(', '.join(tags))\n",
    "\n",
    "        # text\n",
    "        text = []\n",
    "        full_text = article.find(\n",
    "        'div',\n",
    "        {\n",
    "            'xmlns': 'http://www.w3.org/1999/xhtml'\n",
    "        })\n",
    "        try:\n",
    "            for elem in full_text:\n",
    "                text.append(elem.text)\n",
    "            text = '\\n'.join(text)\n",
    "        except AttributeError:\n",
    "            text = full_text.text\n",
    "        articles_data['text'].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_data['link'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.DataFrame(articles_data)\n",
    "\n",
    "articles_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['link'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold standard\n",
    "Update dataset for our keywords and gold standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Suddenly, a new article appeared...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df[1:6]\n",
    "\n",
    "articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check length of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = articles_df['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 0\n",
    "for text in texts:\n",
    "    length += len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* New dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Keywords selection methodology__:\n",
    "* New keywords are selected manually. To do that we must read the whole article.\n",
    "* Keywords in this case is a list of expressions that can help to search this article in the Web. In the same time the list can be perceived like summary of the text (due to the specific of the texts).\n",
    "* These keywords must be presented in the text of the article.\n",
    "* List contains synonymes (ex. *NLP* and *обработка естественного языка*). \n",
    "* Keywords consist from words and collocations (n-grams).\n",
    "* Term can appear 1 time in the text but in fact it represents the important information about the text.\n",
    "* Collocations are not directly extracted from text (not 'as is'). Head words were given to the nominative, but collocates were agreed in number and case. There are not lemmas but collocations.\n",
    "* Keywords are in Russian and English due to the specific of the texts (*technologies*, *informational technologies*, *programming*, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*See more about markup and gold standard in the comments below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_keywords.txt', 'r', encoding='utf-8') as f:\n",
    "    new_tags = f.read().split('\\n')[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['new tags'] = new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comparative analysis of the keyword lists:__\n",
    "* Original tags contain tags that are not indicated in the text directly but they are related with the text and can improve informational retrieval. Our keywords only contain expressions from the texts.\n",
    "* Original tags are mostly in the lower case. In the list of the new tags we can see all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to prepare the gold standard?__\n",
    "* Take our keywords and add it to orginal tags. \n",
    "* Remove words and expressions that are not presented in the original text.\n",
    "* Do all these actions manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions?__\n",
    "* __Why manually?__ This is gold standard, and the gold set must be close to ideal. Humans are native speakers of the language and could resolve many language problems better then machine.\n",
    "* __Why we remove the keywords that are not in the text?__ RAKE and TextRank are based on the words of the one particular text. That is why we can not include in the gold set absent expressions.\n",
    "* __Why we keep the terms on English?__ Because we can rank the expressions regardless the language. BERT-Encoder can be based on the multilingual set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['original tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tags = [[expression.lower() for expression in text] for text in articles_df['original tags'].str.split(', ')]\n",
    "new_tags = [[expression.lower() for expression in text] for text in articles_df['new tags'].str.split(', ')]\n",
    "\n",
    "print(original_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_tags = []\n",
    "for orig, new in zip(original_tags, new_tags):\n",
    "    gold = set(orig + new)\n",
    "    gold_tags.append(', '.join(gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_tags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['gold tags'] = gold_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemmatized gold tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gold lemmatized.txt', 'r', encoding='utf-8') as fl:\n",
    "    gold_lemmatized = fl.read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['lemmatized gold tags'] = gold_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemmatized gold tags with POS-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gold lemmatized pos.txt', 'r', encoding='utf-8') as flp:\n",
    "    gold_lemmatized_pos = flp.read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['lemmatized pos gold tags'] = gold_lemmatized_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Length of the gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['keywords length'] = articles_df['gold tags'].str.split(', ').str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save keywords that correspond to patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequent patterns are following:\n",
    "* NOUN (*технология*, *хакатон*)\n",
    "* PROPN (*SberDevices*, *ruBERT*)\n",
    "* ADJ + NOUN (*нейронная сеть*, *машинное обучение*)\n",
    "* NOUN + NOUN (*распознавание речи*, *обработка текста*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGS = [['NOUN'], ['PROPN'], ['ADJ', 'NOUN'], ['NOUN', 'NOUN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_keywords = []\n",
    "for text in gold_lemmatized_pos:\n",
    "    new_text = []\n",
    "    for keyword in text.split(', '):\n",
    "        pos_list = []\n",
    "        for word in keyword.split():\n",
    "            pos = word.split('_')[1]\n",
    "            pos_list.append(pos)\n",
    "        if pos_list in POS_TAGS:\n",
    "            new_text.append(keyword)\n",
    "    new_text = ', '.join(new_text)\n",
    "    pattern_keywords.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['patterns gold tags'] = pattern_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>original tags</th>\n",
       "      <th>text</th>\n",
       "      <th>new tags</th>\n",
       "      <th>gold tags</th>\n",
       "      <th>keywords length</th>\n",
       "      <th>lemmatized gold tags</th>\n",
       "      <th>lemmatized pos gold tags</th>\n",
       "      <th>patterns gold tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://habr.com/ru/company/just_ai/news/t/698...</td>\n",
       "      <td>искусственный интеллект, голосовые интерфейсы,...</td>\n",
       "      <td>2 декабря в Москве в онлайн- и офлайн-формате ...</td>\n",
       "      <td>Conversations, конференция, NLP, диалоговые пл...</td>\n",
       "      <td>conversations, deeppavlov, sberdevices, yandex...</td>\n",
       "      <td>23</td>\n",
       "      <td>conversations, deeppavlov, sberdevices, yandex...</td>\n",
       "      <td>conversations_PROPN, deeppavlov_PROPN, sberdev...</td>\n",
       "      <td>conversations_PROPN, deeppavlov_PROPN, sberdev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://habr.com/ru/company/first/blog/699380/</td>\n",
       "      <td>open source, ai, ml, инструменты разработчика,...</td>\n",
       "      <td>\\nОт создания дипфейков до обработки естествен...</td>\n",
       "      <td>технологии, опенсорс, машинное обучение, глубо...</td>\n",
       "      <td>технологии, нейронная сеть, приложение, ai, гл...</td>\n",
       "      <td>28</td>\n",
       "      <td>технология, нейронный сеть, приложение, ai, гл...</td>\n",
       "      <td>технология_NOUN, нейронный_ADJ сеть_NOUN, прил...</td>\n",
       "      <td>технология_NOUN, нейронный_ADJ сеть_NOUN, прил...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://habr.com/ru/company/alfa/blog/698660/</td>\n",
       "      <td>хакатон, граф, машинное обучение, нейросети, с...</td>\n",
       "      <td>В сентябре 2022 проходил хакатон «Машинное обу...</td>\n",
       "      <td>хакатон, графы, VK, ВК, Альфа-Банк, машинное о...</td>\n",
       "      <td>1 место, хакатон, графы, альфа-банк, обработка...</td>\n",
       "      <td>13</td>\n",
       "      <td>1 место, хакатон, граф, альфа-банк, обработка ...</td>\n",
       "      <td>1_NUM место_NOUN, хакатон_NOUN, граф_NOUN, аль...</td>\n",
       "      <td>хакатон_NOUN, граф_NOUN, альфа-банк_NOUN, обра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://habr.com/ru/company/amvera/blog/699058/</td>\n",
       "      <td>генерирование стихов, генерация текста, создан...</td>\n",
       "      <td>Мне нравится концепция, согласно которой речь ...</td>\n",
       "      <td>обработка естественного языка, NLP, нейронные ...</td>\n",
       "      <td>rubert, сгенерировать стихотворения, lstm, ней...</td>\n",
       "      <td>12</td>\n",
       "      <td>rubert, сгенерировать стихотворение, lstm, ней...</td>\n",
       "      <td>rubert_PROPN, сгенерировать_VERB стихотворение...</td>\n",
       "      <td>rubert_PROPN, lstm_PROPN, нейронный_ADJ сеть_N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://habr.com/ru/company/unidata/blog/698268/</td>\n",
       "      <td>entity resolution, data matching, record linka...</td>\n",
       "      <td>Всем привет!\\nМы хотим рассказать немного об e...</td>\n",
       "      <td>entity resolution, дедупликация, повторы, дубл...</td>\n",
       "      <td>дубликаты, нейросети, entity resolution, data ...</td>\n",
       "      <td>16</td>\n",
       "      <td>дубликат, нейросеть, entity resolution, data m...</td>\n",
       "      <td>дубликат_NOUN, нейросеть_NOUN, entity_NOUN res...</td>\n",
       "      <td>дубликат_NOUN, нейросеть_NOUN, entity_NOUN res...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://habr.com/ru/company/just_ai/news/t/698...   \n",
       "1     https://habr.com/ru/company/first/blog/699380/   \n",
       "2      https://habr.com/ru/company/alfa/blog/698660/   \n",
       "3    https://habr.com/ru/company/amvera/blog/699058/   \n",
       "4   https://habr.com/ru/company/unidata/blog/698268/   \n",
       "\n",
       "                                       original tags  \\\n",
       "0  искусственный интеллект, голосовые интерфейсы,...   \n",
       "1  open source, ai, ml, инструменты разработчика,...   \n",
       "2  хакатон, граф, машинное обучение, нейросети, с...   \n",
       "3  генерирование стихов, генерация текста, создан...   \n",
       "4  entity resolution, data matching, record linka...   \n",
       "\n",
       "                                                text  \\\n",
       "0  2 декабря в Москве в онлайн- и офлайн-формате ...   \n",
       "1  \\nОт создания дипфейков до обработки естествен...   \n",
       "2  В сентябре 2022 проходил хакатон «Машинное обу...   \n",
       "3  Мне нравится концепция, согласно которой речь ...   \n",
       "4  Всем привет!\\nМы хотим рассказать немного об e...   \n",
       "\n",
       "                                            new tags  \\\n",
       "0  Conversations, конференция, NLP, диалоговые пл...   \n",
       "1  технологии, опенсорс, машинное обучение, глубо...   \n",
       "2  хакатон, графы, VK, ВК, Альфа-Банк, машинное о...   \n",
       "3  обработка естественного языка, NLP, нейронные ...   \n",
       "4  entity resolution, дедупликация, повторы, дубл...   \n",
       "\n",
       "                                           gold tags  keywords length  \\\n",
       "0  conversations, deeppavlov, sberdevices, yandex...               23   \n",
       "1  технологии, нейронная сеть, приложение, ai, гл...               28   \n",
       "2  1 место, хакатон, графы, альфа-банк, обработка...               13   \n",
       "3  rubert, сгенерировать стихотворения, lstm, ней...               12   \n",
       "4  дубликаты, нейросети, entity resolution, data ...               16   \n",
       "\n",
       "                                lemmatized gold tags  \\\n",
       "0  conversations, deeppavlov, sberdevices, yandex...   \n",
       "1  технология, нейронный сеть, приложение, ai, гл...   \n",
       "2  1 место, хакатон, граф, альфа-банк, обработка ...   \n",
       "3  rubert, сгенерировать стихотворение, lstm, ней...   \n",
       "4  дубликат, нейросеть, entity resolution, data m...   \n",
       "\n",
       "                            lemmatized pos gold tags  \\\n",
       "0  conversations_PROPN, deeppavlov_PROPN, sberdev...   \n",
       "1  технология_NOUN, нейронный_ADJ сеть_NOUN, прил...   \n",
       "2  1_NUM место_NOUN, хакатон_NOUN, граф_NOUN, аль...   \n",
       "3  rubert_PROPN, сгенерировать_VERB стихотворение...   \n",
       "4  дубликат_NOUN, нейросеть_NOUN, entity_NOUN res...   \n",
       "\n",
       "                                  patterns gold tags  \n",
       "0  conversations_PROPN, deeppavlov_PROPN, sberdev...  \n",
       "1  технология_NOUN, нейронный_ADJ сеть_NOUN, прил...  \n",
       "2  хакатон_NOUN, граф_NOUN, альфа-банк_NOUN, обра...  \n",
       "3  rubert_PROPN, lstm_PROPN, нейронный_ADJ сеть_N...  \n",
       "4  дубликат_NOUN, нейросеть_NOUN, entity_NOUN res...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.to_csv('articles_df.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It seems that only our tags remained.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract keywords\n",
    "Extract keywords from the parsed texts with RAKE, TextRank and BERT-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocessing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = articles_df['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = []\n",
    "for text in texts:\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('•', '', text)\n",
    "    cleaned_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"2 декабря в Москве в онлайн- и офлайн-формате состоится Conversations – ежегодная конференция по разговорному AI для разработчиков и бизнеса. Про NLP-сервисы, диалоговые платформы и фреймворки, синтез и распознавание речи, UX и проектирование разговорных интерфейсов, генеративные модели и не только расскажут KODE, MTS AI, Альфа-Банк, Сбер, Yandex Cloud, DeepPavlov и другие эксперты. В нашем анонсе – особо интригующие спойлеры и промокод на скидку. Участников конференции ждет 2 потока докладов – Business и Technology. С полной программой можно познакомиться на сайте Conversations, ну а мы предлагаем погрузиться именно в Technology Track. Секция «Good morning, ML!» Что делать, если исходящий звонок колл-центра попадает на голосовую почту или умного ассистента? О технологиях ML для создания модели идентификации голосовой почты и виртуального помощника расскажет Артем Бондарь, Voximplant.Мурат Апишев из Just AI раскроет детали работы над индустриальной NLP-платформой: как организовали управление NLP-сервисами и моделями, стандартизировали встраивание собственных и open-source решений и выучили свой управляемый парафраз для помощи пользователям.Как сделать модель, которая понимает всех — от тёти Сары до фрау Заурих, и экономично использует вычислительные ресурсы? Ответы готовы у Антона Ермилова, Yandex Cloud. Мария Тихонова, SberDevices, проведет экскурсию по «Зоопарку генеративных моделей 2022» и расскажет про все современные инструменты работы с текстом на основе генеративных моделей. Секция «Bot's developing: от сценария к личности» Никита Блинков из VK расскажет о подходах к измерению и развитию умности голосового помощника: выделение крупных классов запросов, улучшение качества ответов по классам и группам внутри них, а также автоматический набор входных фраз для отдельных интентов.Что важнее при разработке личности ассистента – ML, психология или сценаристика? Ангелина Большина и Галина Прохорова из MTS AI покажут многопрофильный подход к разработке личности и раскроют инсайты из своего исследования (например, почему технические возможности тонкой настройки личности в функционале, который основан не на правилах, а на ML-инструментах, ограничены?). Классические блок-схемы устарели? Почему древовидные схемы не подходят для проектирования сложных разговорных сценариев? Об альтернативах – AirTable, Fable и эволюционном подходе к проектированию блок-схем с помощью языка Дракон поговорим с Кириллом Богатовым, KODE. Секция «Set tool. Инструменты и практики» Как объединить высокую управляемость сценарного подхода в комбинации с использованием генеративных языковых моделей? Об управлении через автоматическое построение диалоговых графов и контроль диалога с помощью PALs и Grounding Knowledge расскажут Денис Кузнецов и Данила Корнев, DeepPavlov.Андрей Татаринов из AGIMA AI поделится инсайтами работы с фреймворком RASA и расскажет, как сделать бота с 1500+ интентами и иерархическим классификатором.Про опыт трансформации из legacy-систем в единое платформенное решение расскажут спикеры Альфа-Банка Наталья Балыбердина и Станислав Милых. Ребята научат, как создать целевую инфраструктуру для ботов в банке и какую аналитику учесть при планировании виртуального помощника в чате. Больше спикеров и тем – на сайте Conversations! Присоединяйтесь! Участвовать можно и офлайн, и онлайн. Промокод на скидку 10% для читателей Хабра: CNVS22_UnR, билеты здесь Узнать о том, как это было в прошлом году, можете по ссылке. \""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 01:48:18 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2022-11-20 01:48:18 INFO: Use device: cpu\n",
      "2022-11-20 01:48:18 INFO: Loading: tokenize\n",
      "2022-11-20 01:48:19 INFO: Loading: pos\n",
      "2022-11-20 01:48:19 INFO: Loading: lemma\n",
      "2022-11-20 01:48:19 INFO: Loading: depparse\n",
      "2022-11-20 01:48:20 INFO: Loading: ner\n",
      "2022-11-20 01:48:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_nlp(text: str):\n",
    "    '''\n",
    "    Get lemmatized text with stanza.\n",
    "    '''\n",
    "\n",
    "    text_lemmas = []\n",
    "    text_lemmas_pos = []\n",
    "\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.pos != 'PUNCT' and word.lemma not in rus_sw:\n",
    "                text_lemmas.append(word.lemma.lower())\n",
    "                text_lemmas_pos.append(f'{word.lemma.lower()}_{word.pos}')\n",
    "\n",
    "    return ' '.join(text_lemmas), ' '.join(text_lemmas_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [[], []]\n",
    "for text in cleaned_texts:\n",
    "    processed = stanza_nlp(text)\n",
    "    for container, lemmatized in zip(lemmas, processed):\n",
    "        container.append(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"2 декабрь москва онлайн офлайн формат состояться conversations ежегодный конференция разговорный ai разработчик бизнес nlp сервис диалоговый платформа фреймворка синтез распознавание речь ux проектирование разговорный интерфейс генеративный модель рассказать kode mts ai альфа-банк сбер yandex cloud deeppavlov эксперт наш анонсе особо интриговать спойлера промокод скидка участник конференция ждать 2 поток доклад business technology полный программа знакомиться сайт conversations предлагать погрузиться именно technology track секция good morning ml делать исходить звонок колл-центр попадать голосовой почта умный ассистент технология ml создание модель идентификация голосовой почта виртуальный помощник рассказать артем бондарь voximplant.мурат апишев just ai раскроить деталь работа индустриальный nlp платформа организовывать управление nlp сервис модель стандартизировать встраивание собственный open-source решение выучить свой управлять парафраз помощь пользователь делать модель который понимать ребенок сара фрай заурих экономично использовать вычислительный ресурс ответ готовый антон ермилов yandex cloud мария тихонова sberdevices провести экскурсия зоопарка генеративный модель 2022 рассказать весь современный инструмент работа текст основа генеративный модель секция bot's developing сценарий личность никита блинков vk рассказать подход измерение развитие умность голосовой помощник выделение крупный класс запрос улучшение качество ответ класс группа внутри также автоматический набор входной фраза отдельный интент важный разработка личность ассистент ml психология сценаристика ангелин большина галина прохоров mts ai показать многопрофильный подход разработка личность раскрыть инсайт свой исследование например почему технический возможность тонкий настройка личность функционал который основать правило ml инструмент ограничить классический блок схема устареть почему древовидный схема подходить проектирование сложный разговорный сценарий альтернатива airtable fable эволюционный подход проектирование блок-сх помощь язык дракон говорить кирилл богатов kode секция set tool инструмент практика объединить высокий управляемость сценарный подход комбинация использование генеративный языковой модель управление автоматический построение диалоговый граф контроль диалог помощь pals grounding knowledge рассказать денис кузнецов данила корнев deeppavlov андрей татаринов agima ai делиться инсайт работа фреймворкое rasa рассказать делать бота 1500+ интент иерархический классификатор опыт трансформация legacy система единый платформенный решение рассказать спикер альфа-банка наталья балыбердин станислав милых ребята научить создать целевой инфраструктура бот банк аналитик учесть планирование виртуальный помощник чата спикер сайт conversations присоединяться участвовать офлайн онлайн промокод скидка 10 % читатель хабр cnvs22_unr билет узнать это прошлый год мочь ссылка\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0][0]  # lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"2_NUM декабрь_NOUN москва_PROPN онлайн_NOUN офлайн_NOUN формат_NOUN состояться_VERB conversations_PROPN ежегодный_ADJ конференция_NOUN разговорный_ADJ ai_PROPN разработчик_NOUN бизнес_NOUN nlp_PROPN сервис_NOUN диалоговый_ADJ платформа_NOUN фреймворка_NOUN синтез_NOUN распознавание_NOUN речь_NOUN ux_PROPN проектирование_NOUN разговорный_ADJ интерфейс_NOUN генеративный_ADJ модель_NOUN рассказать_VERB kode_PROPN mts_PROPN ai_PROPN альфа-банк_PROPN сбер_PROPN yandex_PROPN cloud_PROPN deeppavlov_PROPN эксперт_NOUN наш_DET анонсе_NOUN особо_ADV интриговать_VERB спойлера_NOUN промокод_NOUN скидка_NOUN участник_NOUN конференция_NOUN ждать_VERB 2_NUM поток_NOUN доклад_NOUN business_PROPN technology_PROPN полный_ADJ программа_NOUN знакомиться_VERB сайт_NOUN conversations_PROPN предлагать_VERB погрузиться_VERB именно_PART technology_PROPN track_PROPN секция_NOUN good_PROPN morning_PROPN ml_PROPN делать_VERB исходить_VERB звонок_NOUN колл-центр_NOUN попадать_VERB голосовой_ADJ почта_NOUN умный_ADJ ассистент_NOUN технология_NOUN ml_PROPN создание_NOUN модель_NOUN идентификация_NOUN голосовой_ADJ почта_NOUN виртуальный_ADJ помощник_NOUN рассказать_VERB артем_PROPN бондарь_PROPN voximplant.мурат_PROPN апишев_PROPN just_PROPN ai_PROPN раскроить_VERB деталь_NOUN работа_NOUN индустриальный_ADJ nlp_PROPN платформа_NOUN организовывать_VERB управление_NOUN nlp_PROPN сервис_NOUN модель_NOUN стандартизировать_VERB встраивание_NOUN собственный_ADJ open-source_PROPN решение_NOUN выучить_VERB свой_DET управлять_VERB парафраз_NOUN помощь_NOUN пользователь_NOUN делать_VERB модель_NOUN который_PRON понимать_VERB ребенок_NOUN сара_PROPN фрай_NOUN заурих_PROPN экономично_ADV использовать_VERB вычислительный_ADJ ресурс_NOUN ответ_NOUN готовый_ADJ антон_PROPN ермилов_PROPN yandex_PROPN cloud_PROPN мария_PROPN тихонова_PROPN sberdevices_PROPN провести_VERB экскурсия_NOUN зоопарка_NOUN генеративный_ADJ модель_NOUN 2022_NUM рассказать_VERB весь_DET современный_ADJ инструмент_NOUN работа_NOUN текст_NOUN основа_NOUN генеративный_ADJ модель_NOUN секция_NOUN bot's_PROPN developing_PROPN сценарий_NOUN личность_NOUN никита_PROPN блинков_PROPN vk_PROPN рассказать_VERB подход_NOUN измерение_NOUN развитие_NOUN умность_NOUN голосовой_ADJ помощник_NOUN выделение_NOUN крупный_ADJ класс_NOUN запрос_NOUN улучшение_NOUN качество_NOUN ответ_NOUN класс_NOUN группа_NOUN внутри_ADV также_PART автоматический_ADJ набор_NOUN входной_ADJ фраза_NOUN отдельный_ADJ интент_NOUN важный_ADJ разработка_NOUN личность_NOUN ассистент_NOUN ml_PROPN психология_NOUN сценаристика_NOUN ангелин_PROPN большина_PROPN галина_PROPN прохоров_PROPN mts_PROPN ai_PROPN показать_VERB многопрофильный_ADJ подход_NOUN разработка_NOUN личность_NOUN раскрыть_VERB инсайт_NOUN свой_DET исследование_NOUN например_ADV почему_ADV технический_ADJ возможность_NOUN тонкий_ADJ настройка_NOUN личность_NOUN функционал_NOUN который_PRON основать_VERB правило_NOUN ml_PROPN инструмент_NOUN ограничить_VERB классический_ADJ блок_NOUN схема_NOUN устареть_VERB почему_ADV древовидный_ADJ схема_NOUN подходить_VERB проектирование_NOUN сложный_ADJ разговорный_ADJ сценарий_NOUN альтернатива_NOUN airtable_PROPN fable_PROPN эволюционный_ADJ подход_NOUN проектирование_NOUN блок-сх_NOUN помощь_NOUN язык_NOUN дракон_PROPN говорить_VERB кирилл_PROPN богатов_PROPN kode_PROPN секция_NOUN set_PROPN tool_PROPN инструмент_NOUN практика_NOUN объединить_VERB высокий_ADJ управляемость_NOUN сценарный_ADJ подход_NOUN комбинация_NOUN использование_NOUN генеративный_ADJ языковой_ADJ модель_NOUN управление_NOUN автоматический_ADJ построение_NOUN диалоговый_ADJ граф_NOUN контроль_NOUN диалог_NOUN помощь_NOUN pals_PROPN grounding_PROPN knowledge_PROPN рассказать_VERB денис_PROPN кузнецов_PROPN данила_PROPN корнев_PROPN deeppavlov_PROPN андрей_PROPN татаринов_PROPN agima_PROPN ai_PROPN делиться_VERB инсайт_NOUN работа_NOUN фреймворкое_NOUN rasa_PROPN рассказать_VERB делать_VERB бота_NOUN 1500+_NUM интент_NOUN иерархический_ADJ классификатор_NOUN опыт_NOUN трансформация_NOUN legacy_PROPN система_NOUN единый_ADJ платформенный_ADJ решение_NOUN рассказать_VERB спикер_NOUN альфа-банка_PROPN наталья_PROPN балыбердин_PROPN станислав_PROPN милых_PROPN ребята_NOUN научить_VERB создать_VERB целевой_ADJ инфраструктура_NOUN бот_NOUN банк_NOUN аналитик_NOUN учесть_VERB планирование_NOUN виртуальный_ADJ помощник_NOUN чата_NOUN спикер_NOUN сайт_NOUN conversations_PROPN присоединяться_VERB участвовать_VERB офлайн_NOUN онлайн_NOUN промокод_NOUN скидка_NOUN 10_NUM %_SYM читатель_NOUN хабр_PROPN cnvs22_unr_PROPN билет_NOUN узнать_VERB это_PRON прошлый_ADJ год_NOUN мочь_VERB ссылка_NOUN\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[1][0]  # lemmas with pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGS_LOWER = [['noun'], ['propn'], ['adj', 'noun'], ['noun', 'noun']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patterns(text: str) -> str:\n",
    "    '''\n",
    "    Get patterns from keywords text.\n",
    "    '''\n",
    "\n",
    "    if text == '':\n",
    "\n",
    "        return ''\n",
    "\n",
    "    new_text = []\n",
    "\n",
    "    for keyword in text.split(', '):\n",
    "        pos_list = []\n",
    "        keywords = keyword.split()\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                pos = word.split('_')[1]\n",
    "                pos_list.append(pos)\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if len(pos_list) == len(keywords):\n",
    "                if pos_list in POS_TAGS_LOWER or pos_list in POS_TAGS:\n",
    "                    new_text.append(keyword)\n",
    "\n",
    "    new_text = ', '.join(new_text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __RAKE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rake = RAKE.Rake(list(rus_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Already without stopwords from the set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On the raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_raw(func: Callable, texts: list, mode='summa', **kwargs) -> list:\n",
    "    '''\n",
    "    Keyword extraction for all of the texts in the corpora given\n",
    "    with the extractors on the whole texts.\n",
    "    '''\n",
    "\n",
    "    keywords = []\n",
    "\n",
    "    for text in texts:\n",
    "        if mode == 'rake':\n",
    "            coll_list = ', '.join([coll[0] for coll in func(text, **kwargs)])\n",
    "        else:\n",
    "            coll_list = ', '.join(func(text, **kwargs).split('\\n'))\n",
    "        keywords.append(coll_list)\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max: trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'организовали управление nlp-сервисами, классические блок-схемы устарели, офлайн-формате состоится conversations, почему древовидные схемы, проектированию блок-схем, проектирование разговорных интерфейсов, особо интригующие спойлеры, предлагаем погрузиться именно, индустриальной nlp-платформой, стандартизировали встраивание собственных, open-source решений, основе генеративных моделей, улучшение качества ответов, создать целевую инфраструктуру, какую аналитику учесть, планировании виртуального помощника, секция «good morning, личности» никита блинков, секция «set tool, современные инструменты работы, разработке личности ассистента, разработке личности, nlp-сервисы, сайте conversations, секция «bot, умного ассистента, ежегодная конференция, разговорному ai, диалоговые платформы, распознавание речи, генеративные модели, mts ai, альфа-банк, yandex cloud, другие эксперты, нашем анонсе, полной программой, голосовую почту, мурат апишев, помощи пользователям, сделать модель, которая понимает, тёти сары, фрау заурих, ответы готовы, антона ермилова, мария тихонова, проведет экскурсию, s developing, группам внутри, отдельных интентов, ангелина большина, галина прохорова, раскроют инсайты, своего исследования, который основан, эволюционном подходе, кириллом богатовым, контроль диалога, помощью pals, данила корнев, андрей татаринов, фреймворком rasa, сделать бота, иерархическим классификатором, опыт трансформации, legacy-систем, станислав милых, ребята научат, читателей хабра, прошлом году, расскажут kode, technology track, технологиях ml, ml-инструментах, vk расскажет, офлайн, инструменты, technology, ml, kode, расскажет, 2 декабря, москве, онлайн-, разработчиков, бизнеса, фреймворки, синтез, ux, сбер, deeppavlov, промокод, скидку, business, познакомиться, делать, voximplant, моделями, sberdevices, текстом, сценария, подходах, измерению, классам, важнее, психология, сценаристика, например, функционале, правилах, ограничены, подходят, альтернативах, airtable, fable, практики», комбинации, управлении, 1500+ интентами, ботов, банке, чате, спикеров, присоединяйтесь, участвовать, онлайн, скидку 10%, cnvs22_unr, билеты, узнать, это, можете, ссылке, », —'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_raw_3 = extract_from_raw(Rake.run, cleaned_texts, mode='rake', maxWords=3)\n",
    "\n",
    "rake_raw_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_lemmas_3 = extract_from_raw(Rake.run, lemmas[0], mode='rake', maxWords=3)\n",
    "\n",
    "rake_lemmas_3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*:(*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_lemmas_pos_3 = extract_from_raw(Rake.run, lemmas[1], mode='rake', maxWords=3)\n",
    "\n",
    "rake_lemmas_pos_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_lemmas_pos_patt_3 = []\n",
    "for text in rake_lemmas_pos_3:\n",
    "    pattern_text = get_patterns(text)\n",
    "    rake_lemmas_pos_patt_3.append(pattern_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_lemmas_pos_patt_3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max: bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'open-source решений, проектированию блок-схем, индустриальной nlp-платформой, nlp-сервисы, ежегодная конференция, разговорному ai, диалоговые платформы, распознавание речи, генеративные модели, mts ai, альфа-банк, yandex cloud, другие эксперты, нашем анонсе, полной программой, сайте conversations, голосовую почту, умного ассистента, мурат апишев, помощи пользователям, сделать модель, которая понимает, тёти сары, фрау заурих, ответы готовы, антона ермилова, мария тихонова, проведет экскурсию, секция «bot, s developing, группам внутри, отдельных интентов, ангелина большина, галина прохорова, разработке личности, раскроют инсайты, своего исследования, который основан, эволюционном подходе, кириллом богатовым, контроль диалога, помощью pals, данила корнев, андрей татаринов, фреймворком rasa, сделать бота, иерархическим классификатором, опыт трансформации, legacy-систем, станислав милых, ребята научат, читателей хабра, прошлом году, расскажут kode, technology track, технологиях ml, ml-инструментах, vk расскажет, technology, ml, kode, расскажет, 2 декабря, москве, онлайн-, разработчиков, бизнеса, фреймворки, синтез, ux, сбер, deeppavlov, промокод, скидку, business, познакомиться, делать, voximplant, моделями, sberdevices, текстом, сценария, подходах, измерению, классам, важнее, психология, сценаристика, например, функционале, правилах, ограничены, подходят, альтернативах, airtable, fable, инструменты, практики», комбинации, управлении, 1500+ интентами, ботов, банке, чате, спикеров, присоединяйтесь, участвовать, офлайн, онлайн, скидку 10%, cnvs22_unr, билеты, узнать, это, можете, ссылке, », —'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_raw_2 = extract_from_raw(Rake.run, cleaned_texts, mode='rake', maxWords=2)\n",
    "\n",
    "rake_raw_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_lemmas_2 = extract_from_raw(Rake.run, lemmas[0], mode='rake', maxWords=2)\n",
    "\n",
    "rake_lemmas_2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*:(*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_lemmas_pos_2 = extract_from_raw(Rake.run, lemmas[1], mode='rake', maxWords=2)\n",
    "\n",
    "rake_lemmas_pos_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_lemmas_pos_patt_2 = []\n",
    "for text in rake_lemmas_pos_2:\n",
    "    pattern_text = get_patterns(text)\n",
    "    rake_lemmas_pos_patt_2.append(pattern_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_lemmas_pos_patt_2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __TextRank__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'моделями, ai, генеративные модели, генеративных моделей, модель которая, секция, ml, расскажут, инструменты, conversations, который, разговорному, разговорных, личности, nlp диалоговые, диалоговых, голосовую, голосовой, голосового, помощи, помощью, почему, kode, помощника расскажет, свой, своего, ответы, ответов, автоматический, автоматическое, фреймворки, фреймворком, аналитику, ангелина, airtable, году, целевую, речи, погрузиться, подходах, подход, подходят, подходе, подхода, конференция, конференции, deeppavlov, решений, решение, yandex, cloud, ассистента, банке, милых, ребята, инсайты, инсайтами, анонсе, звонок, хабра, интригующие'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa_raw = extract_from_raw(summa.keywords.keywords, cleaned_texts, language='russian', additional_stopwords=rus_sw)\n",
    "\n",
    "summa_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'подход, подходить, генеративный модель рассказать kode, ml делать, помощь, конференция разговорный ai, инструмент, секция, работа, conversations, сценарий личность, голосовой, помощник, проектирование, ответ, бот банк, technology, nlp, класс, почему, интент, решение, автоматический, свой, который, бота, блок схема, yandex cloud deeppavlov, инсайт, фреймворкое, диалоговый платформа фреймворка, спикер, ассистент, управление, онлайн офлайн, промокод скидка, мочь, сайт'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa_lemmas = extract_from_raw(summa.keywords.keywords, lemmas[0], language='russian')\n",
    "\n",
    "summa_lemmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рассказать_verb, модель_noun, ai_propn, _num, подход_noun, ml_propn, личность_noun, помощь_noun, инструмент_noun, секция_noun, проектирование_noun, генеративный_adj, работа_noun, разговорный_adj, делать_verb, conversations_propn, голосовой_adj, помощник_noun, deeppavlov_propn, ответ_noun, nlp_propn, класс_noun, technology_propn, почему_adv, схема_noun, интент_noun, решение_noun, свой_det, автоматический_adj, который_pron, бота_noun, бот_noun, сценарий_noun, конференция_noun, диалоговый_adj, kode_propn, инсайт_noun, фреймворка_noun, фреймворкое_noun, спикер_noun, ассистент_noun, платформа_noun, управление_noun, yandex_propn, cloud_propn, офлайн_noun, мочь_verb, онлайн_noun, промокод_noun, год_noun, сайт_noun, виртуальный_adj'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa_lemmas_pos = extract_from_raw(summa.keywords.keywords, lemmas[1], language='russian')\n",
    "\n",
    "summa_lemmas_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa_lemmas_pos_patt = []\n",
    "for text in summa_lemmas_pos:\n",
    "    pattern_text = get_patterns(text)\n",
    "    summa_lemmas_pos_patt.append(pattern_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'модель_noun, ai_propn, подход_noun, ml_propn, личность_noun, помощь_noun, инструмент_noun, секция_noun, проектирование_noun, работа_noun, conversations_propn, помощник_noun, deeppavlov_propn, ответ_noun, nlp_propn, класс_noun, technology_propn, схема_noun, интент_noun, решение_noun, бота_noun, бот_noun, сценарий_noun, конференция_noun, kode_propn, инсайт_noun, фреймворка_noun, фреймворкое_noun, спикер_noun, ассистент_noun, платформа_noun, управление_noun, yandex_propn, cloud_propn, офлайн_noun, онлайн_noun, промокод_noun, год_noun, сайт_noun'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa_lemmas_pos_patt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __TF-IDF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(articles_df['keywords length'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take first 30 keywords from TF-IDF because it does not provide fixed number of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_index(texts: list) -> csr_matrix:\n",
    "    '''\n",
    "    Compute inverted index in the form of a matrix.\n",
    "    '''\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), stop_words=rus_sw)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    X = X.toarray()\n",
    "\n",
    "    return vectorizer, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_tf_idf(index: csr_matrix, vectorizer: TfidfVectorizer):\n",
    "    '''\n",
    "    Get the keywords from index by sorting ngrams by TF-IDF scores.\n",
    "    '''\n",
    "    feature_array = np.array(vectorizer.get_feature_names_out())\n",
    "    keywords = []\n",
    "\n",
    "    for row in index:\n",
    "        sorted_nums = np.argsort(row).flatten()[::-1]\n",
    "        sorted_words = ', '.join(list(feature_array[sorted_nums].ravel()[:30]))\n",
    "        keywords.append(sorted_words)\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai, расскажет, личности, conversations, секция, расскажут, генеративных, помощника, ml, kode, онлайн, сайте conversations, разработке, ассистента, yandex cloud, yandex, промокод скидку, промокод, technology, разработке личности, виртуального помощника, виртуального, схемы, генеративных моделей, mts ai, mts, скидку, офлайн, моделей, работы'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_vectorizer, raw_index = matrix_index(cleaned_texts)\n",
    "tf_idf_raw = rank_tf_idf(raw_index, raw_vectorizer)\n",
    "\n",
    "tf_idf_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рассказать, ai, генеративный, личность, модель, генеративный модель, секция, проектирование, conversations, разговорный, голосовой, помощник, ml, подход, инструмент, промокод, разработка личность, kode, промокод скидка, сайт conversations, банк, автоматический, yandex cloud, mts ai, mts, technology, ассистент, голосовой почта, скидка, онлайн'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_vectorizer, lemmas_index = matrix_index(lemmas[0])\n",
    "tf_idf_lemmas = rank_tf_idf(lemmas_index, lemmas_vectorizer)\n",
    "\n",
    "tf_idf_lemmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рассказать_verb, ai_propn, личность_noun, генеративный_adj, модель_noun, conversations_propn, проектирование_noun, секция_noun, генеративный_adj модель_noun, разговорный_adj, помощник_noun, голосовой_adj, подход_noun, ml_propn, инструмент_noun, офлайн_noun, сайт_noun conversations_propn, скидка_noun, nlp_propn сервис_noun, разработка_noun личность_noun, technology_propn, голосовой_adj почта_noun, онлайн_noun, ассистент_noun, промокод_noun скидка_noun, спикер_noun, kode_propn, автоматический_adj, yandex_propn, mts_propn'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_pos_vectorizer, lemmas_pos_index = matrix_index(lemmas[1])\n",
    "tf_idf_lemmas_pos = rank_tf_idf(lemmas_pos_index, lemmas_pos_vectorizer)\n",
    "\n",
    "tf_idf_lemmas_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_lemmas_pos_patt = []\n",
    "for text in tf_idf_lemmas_pos:\n",
    "    pattern_text = get_patterns(text)\n",
    "    tf_idf_lemmas_pos_patt.append(pattern_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai_propn, личность_noun, модель_noun, conversations_propn, проектирование_noun, секция_noun, генеративный_adj модель_noun, помощник_noun, подход_noun, ml_propn, инструмент_noun, офлайн_noun, скидка_noun, разработка_noun личность_noun, technology_propn, голосовой_adj почта_noun, онлайн_noun, ассистент_noun, промокод_noun скидка_noun, спикер_noun, kode_propn, yandex_propn, mts_propn'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_lemmas_pos_patt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold tags</th>\n",
       "      <th>textrank</th>\n",
       "      <th>rake trigrams</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conversations, deeppavlov, sberdevices, yandex...</td>\n",
       "      <td>моделями, ai, генеративные модели, генеративны...</td>\n",
       "      <td>организовали управление nlp-сервисами, классич...</td>\n",
       "      <td>ai, расскажет, личности, conversations, секция...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>технологии, нейронная сеть, приложение, ai, гл...</td>\n",
       "      <td>источник, источников, источники, позволяющие, ...</td>\n",
       "      <td>заключение пятнадцать опенсорс-технологий, реш...</td>\n",
       "      <td>license, ии, категория, обучения, особенности,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 место, хакатон, графы, альфа-банк, обработка...</td>\n",
       "      <td>данные, данных, данном, даны, признаков, призн...</td>\n",
       "      <td>использованием tf-idf представлений, последова...</td>\n",
       "      <td>пользователей, признаков, друзей, признаки, по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rubert, сгенерировать стихотворения, lstm, ней...</td>\n",
       "      <td>своё, своих, своей, своим, шаг, шагом, это, эт...</td>\n",
       "      <td>согласно которой речь, обработке естественных ...</td>\n",
       "      <td>евгений, temperature, попробуем, онегин, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>дубликаты, нейросети, entity resolution, data ...</td>\n",
       "      <td>данных, данные, данный, данной, это, эта, этих...</td>\n",
       "      <td>соревнуются state-of-the-art, работоспособном ...</td>\n",
       "      <td>zingg, entity, данных, entity resolution, запи...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           gold tags  \\\n",
       "0  conversations, deeppavlov, sberdevices, yandex...   \n",
       "1  технологии, нейронная сеть, приложение, ai, гл...   \n",
       "2  1 место, хакатон, графы, альфа-банк, обработка...   \n",
       "3  rubert, сгенерировать стихотворения, lstm, ней...   \n",
       "4  дубликаты, нейросети, entity resolution, data ...   \n",
       "\n",
       "                                            textrank  \\\n",
       "0  моделями, ai, генеративные модели, генеративны...   \n",
       "1  источник, источников, источники, позволяющие, ...   \n",
       "2  данные, данных, данном, даны, признаков, призн...   \n",
       "3  своё, своих, своей, своим, шаг, шагом, это, эт...   \n",
       "4  данных, данные, данный, данной, это, эта, этих...   \n",
       "\n",
       "                                       rake trigrams  \\\n",
       "0  организовали управление nlp-сервисами, классич...   \n",
       "1  заключение пятнадцать опенсорс-технологий, реш...   \n",
       "2  использованием tf-idf представлений, последова...   \n",
       "3  согласно которой речь, обработке естественных ...   \n",
       "4  соревнуются state-of-the-art, работоспособном ...   \n",
       "\n",
       "                                              tf-idf  \n",
       "0  ai, расскажет, личности, conversations, секция...  \n",
       "1  license, ии, категория, обучения, особенности,...  \n",
       "2  пользователей, признаков, друзей, признаки, по...  \n",
       "3  евгений, temperature, попробуем, онегин, the, ...  \n",
       "4  zingg, entity, данных, entity resolution, запи...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_raw_df = pd.DataFrame({\n",
    "    'gold tags': articles_df['gold tags'],\n",
    "    'textrank': summa_raw,\n",
    "    'rake trigrams': rake_raw_3,  # save only trigrams because we have trigrams in the gold standard\n",
    "    'tf-idf': tf_idf_raw,\n",
    "    }\n",
    ")\n",
    "\n",
    "results_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_raw_df.to_csv('results_raw.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold tags</th>\n",
       "      <th>textrank</th>\n",
       "      <th>rake trigrams</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conversations, deeppavlov, sberdevices, yandex...</td>\n",
       "      <td>подход, подходить, генеративный модель рассказ...</td>\n",
       "      <td></td>\n",
       "      <td>рассказать, ai, генеративный, личность, модель...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>технология, нейронный сеть, приложение, ai, гл...</td>\n",
       "      <td>модель, моделие, который, инструмент, изображе...</td>\n",
       "      <td>ai источник h2o, ai источник github, github</td>\n",
       "      <td>обучение, изображение, категория, license, ист...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 место, хакатон, граф, альфа-банк, обработка ...</td>\n",
       "      <td>признак, данные, данный, пользователь, последо...</td>\n",
       "      <td>b 496d, 061266620e07n 3 0, e84b0, 9a767, 0be67...</td>\n",
       "      <td>признак, друг, хакатон, пользователь, последов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rubert, сгенерировать стихотворение, lstm, ней...</td>\n",
       "      <td>это, евгение, евгений онегин, онегина, the, мо...</td>\n",
       "      <td>сборник произведение м, ю, как-</td>\n",
       "      <td>евгений, temperature, мочь, онегин, шаг, свой,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>дубликат, нейросеть, entity resolution, data m...</td>\n",
       "      <td>данный, zingg, который, это, любой данные, зап...</td>\n",
       "      <td>16455ntel ми core, fneendeddald4deddaгbu флешк...</td>\n",
       "      <td>zingg, запись, entity, данные, дубликат, entit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           gold tags  \\\n",
       "0  conversations, deeppavlov, sberdevices, yandex...   \n",
       "1  технология, нейронный сеть, приложение, ai, гл...   \n",
       "2  1 место, хакатон, граф, альфа-банк, обработка ...   \n",
       "3  rubert, сгенерировать стихотворение, lstm, ней...   \n",
       "4  дубликат, нейросеть, entity resolution, data m...   \n",
       "\n",
       "                                            textrank  \\\n",
       "0  подход, подходить, генеративный модель рассказ...   \n",
       "1  модель, моделие, который, инструмент, изображе...   \n",
       "2  признак, данные, данный, пользователь, последо...   \n",
       "3  это, евгение, евгений онегин, онегина, the, мо...   \n",
       "4  данный, zingg, который, это, любой данные, зап...   \n",
       "\n",
       "                                       rake trigrams  \\\n",
       "0                                                      \n",
       "1        ai источник h2o, ai источник github, github   \n",
       "2  b 496d, 061266620e07n 3 0, e84b0, 9a767, 0be67...   \n",
       "3                    сборник произведение м, ю, как-   \n",
       "4  16455ntel ми core, fneendeddald4deddaгbu флешк...   \n",
       "\n",
       "                                              tf-idf  \n",
       "0  рассказать, ai, генеративный, личность, модель...  \n",
       "1  обучение, изображение, категория, license, ист...  \n",
       "2  признак, друг, хакатон, пользователь, последов...  \n",
       "3  евгений, temperature, мочь, онегин, шаг, свой,...  \n",
       "4  zingg, запись, entity, данные, дубликат, entit...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lemmas_df = pd.DataFrame({\n",
    "    'gold tags': articles_df['lemmatized gold tags'],\n",
    "    'textrank': summa_lemmas,\n",
    "    'rake trigrams': rake_lemmas_3,\n",
    "    'tf-idf': tf_idf_lemmas\n",
    "    }\n",
    ")\n",
    "\n",
    "results_lemmas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lemmas_df.to_csv('results_lemmas.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas with POS-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold tags</th>\n",
       "      <th>textrank</th>\n",
       "      <th>rake trigrams</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conversations_PROPN, deeppavlov_PROPN, sberdev...</td>\n",
       "      <td>рассказать_verb, модель_noun, ai_propn, _num, ...</td>\n",
       "      <td></td>\n",
       "      <td>рассказать_verb, ai_propn, личность_noun, гене...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>технология_NOUN, нейронный_ADJ сеть_NOUN, прил...</td>\n",
       "      <td>обучение_noun, источник_noun, _num, данные_nou...</td>\n",
       "      <td>ai_propn источник_noun h2o, ai_propn источник_...</td>\n",
       "      <td>обучение_noun, изображение_noun, категория_pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_NUM место_NOUN, хакатон_NOUN, граф_NOUN, аль...</td>\n",
       "      <td>_num, признак_noun, данные_noun, пользователь_...</td>\n",
       "      <td>061266620e07n_num 3_num 0, b_propn 496d, 01000...</td>\n",
       "      <td>признак_noun, друг_noun, хакатон_noun, пользов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rubert_PROPN, сгенерировать_VERB стихотворение...</td>\n",
       "      <td>_propn, мочь_verb, это_pron, свой_det, делать_...</td>\n",
       "      <td>_propn ю, _propn</td>\n",
       "      <td>_propn, евгений_propn, temperature_propn, мочь...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>дубликат_NOUN, нейросеть_NOUN, entity_NOUN res...</td>\n",
       "      <td>_propn, _num, данные_noun, zingg_propn, которы...</td>\n",
       "      <td>first_name_propn =_x r, 16455ntel_num ми_noun ...</td>\n",
       "      <td>_propn, zingg_propn, запись_noun, entity_propn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           gold tags  \\\n",
       "0  conversations_PROPN, deeppavlov_PROPN, sberdev...   \n",
       "1  технология_NOUN, нейронный_ADJ сеть_NOUN, прил...   \n",
       "2  1_NUM место_NOUN, хакатон_NOUN, граф_NOUN, аль...   \n",
       "3  rubert_PROPN, сгенерировать_VERB стихотворение...   \n",
       "4  дубликат_NOUN, нейросеть_NOUN, entity_NOUN res...   \n",
       "\n",
       "                                            textrank  \\\n",
       "0  рассказать_verb, модель_noun, ai_propn, _num, ...   \n",
       "1  обучение_noun, источник_noun, _num, данные_nou...   \n",
       "2  _num, признак_noun, данные_noun, пользователь_...   \n",
       "3  _propn, мочь_verb, это_pron, свой_det, делать_...   \n",
       "4  _propn, _num, данные_noun, zingg_propn, которы...   \n",
       "\n",
       "                                       rake trigrams  \\\n",
       "0                                                      \n",
       "1  ai_propn источник_noun h2o, ai_propn источник_...   \n",
       "2  061266620e07n_num 3_num 0, b_propn 496d, 01000...   \n",
       "3                                   _propn ю, _propn   \n",
       "4  first_name_propn =_x r, 16455ntel_num ми_noun ...   \n",
       "\n",
       "                                              tf-idf  \n",
       "0  рассказать_verb, ai_propn, личность_noun, гене...  \n",
       "1  обучение_noun, изображение_noun, категория_pro...  \n",
       "2  признак_noun, друг_noun, хакатон_noun, пользов...  \n",
       "3  _propn, евгений_propn, temperature_propn, мочь...  \n",
       "4  _propn, zingg_propn, запись_noun, entity_propn...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lemmas_pos_df = pd.DataFrame({\n",
    "    'gold tags': articles_df['lemmatized pos gold tags'],\n",
    "    'textrank': summa_lemmas_pos,\n",
    "    'rake trigrams': rake_lemmas_pos_3,\n",
    "    'tf-idf': tf_idf_lemmas_pos\n",
    "    }\n",
    ")\n",
    "\n",
    "results_lemmas_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lemmas_pos_df.to_csv('results_lemmas_pos.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas with syntactic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold tags</th>\n",
       "      <th>textrank</th>\n",
       "      <th>rake trigrams</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conversations_PROPN, deeppavlov_PROPN, sberdev...</td>\n",
       "      <td>модель_noun, ai_propn, подход_noun, ml_propn, ...</td>\n",
       "      <td></td>\n",
       "      <td>ai_propn, личность_noun, модель_noun, conversa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>технология_NOUN, нейронный_ADJ сеть_NOUN, прил...</td>\n",
       "      <td>обучение_noun, источник_noun, данные_noun, инс...</td>\n",
       "      <td></td>\n",
       "      <td>обучение_noun, изображение_noun, категория_pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_NUM место_NOUN, хакатон_NOUN, граф_NOUN, аль...</td>\n",
       "      <td>признак_noun, данные_noun, пользователь_noun, ...</td>\n",
       "      <td>_propn, 0be67_propn, eaaraa2aara22222222_propn...</td>\n",
       "      <td>признак_noun, друг_noun, хакатон_noun, пользов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rubert_PROPN, сгенерировать_VERB стихотворение...</td>\n",
       "      <td>_propn, онегин_propn, онегина_propn, евгений_p...</td>\n",
       "      <td>_propn</td>\n",
       "      <td>_propn, евгений_propn, temperature_propn, онег...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>дубликат_NOUN, нейросеть_NOUN, entity_NOUN res...</td>\n",
       "      <td>_propn, данные_noun, zingg_propn, запись_noun,...</td>\n",
       "      <td></td>\n",
       "      <td>_propn, zingg_propn, запись_noun, entity_propn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           gold tags  \\\n",
       "0  conversations_PROPN, deeppavlov_PROPN, sberdev...   \n",
       "1  технология_NOUN, нейронный_ADJ сеть_NOUN, прил...   \n",
       "2  1_NUM место_NOUN, хакатон_NOUN, граф_NOUN, аль...   \n",
       "3  rubert_PROPN, сгенерировать_VERB стихотворение...   \n",
       "4  дубликат_NOUN, нейросеть_NOUN, entity_NOUN res...   \n",
       "\n",
       "                                            textrank  \\\n",
       "0  модель_noun, ai_propn, подход_noun, ml_propn, ...   \n",
       "1  обучение_noun, источник_noun, данные_noun, инс...   \n",
       "2  признак_noun, данные_noun, пользователь_noun, ...   \n",
       "3  _propn, онегин_propn, онегина_propn, евгений_p...   \n",
       "4  _propn, данные_noun, zingg_propn, запись_noun,...   \n",
       "\n",
       "                                       rake trigrams  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  _propn, 0be67_propn, eaaraa2aara22222222_propn...   \n",
       "3                                             _propn   \n",
       "4                                                      \n",
       "\n",
       "                                              tf-idf  \n",
       "0  ai_propn, личность_noun, модель_noun, conversa...  \n",
       "1  обучение_noun, изображение_noun, категория_pro...  \n",
       "2  признак_noun, друг_noun, хакатон_noun, пользов...  \n",
       "3  _propn, евгений_propn, temperature_propn, онег...  \n",
       "4  _propn, zingg_propn, запись_noun, entity_propn...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lemmas_pos_patt_df = pd.DataFrame({\n",
    "    'gold tags': articles_df['lemmatized pos gold tags'],\n",
    "    'textrank': summa_lemmas_pos_patt,\n",
    "    'rake trigrams': rake_lemmas_pos_patt_3,\n",
    "    'tf-idf': tf_idf_lemmas_pos_patt\n",
    "    }\n",
    ")\n",
    "\n",
    "results_lemmas_pos_patt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lemmas_pos_patt_df.to_csv('results_lemmas_pos_patt.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate extraction on the sets of extracted words that are in the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(extracted: list, gold: list) -> float:\n",
    "    '''\n",
    "    Compute precision score for the keywords extraction task.\n",
    "    '''\n",
    "    precision = len(set(extracted) & set(gold)) / len(set(extracted))\n",
    "\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(extracted: list, gold: list) -> float:\n",
    "    '''\n",
    "    Compute recall score for the keywords extraction task.\n",
    "    '''\n",
    "    recall = len(set(extracted) & set(gold)) / len(set(gold))\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fscore(precision: float, recall: float) -> float:\n",
    "    '''\n",
    "    Compute F-score score for the keywords extraction task.\n",
    "    '''\n",
    "    if precision == 0 or recall == 0:\n",
    "        f2 = 0\n",
    "    else:\n",
    "        f2 = (2 * recall * precision) / (recall + precision)\n",
    "\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(extracted: list, gold: list) -> list:\n",
    "    '''\n",
    "    Get average scores by metrics for selected method in the list format.\n",
    "    '''\n",
    "\n",
    "    p = []\n",
    "    r = []\n",
    "    f = []\n",
    "    for e_k, g_k in zip(extracted, gold):\n",
    "        e = [ext.lower() for ext in e_k.split(', ')]\n",
    "        g = [gold.lower() for gold in g_k.split(', ')]\n",
    "        precision = get_precision(e, g)\n",
    "        p.append(precision)\n",
    "        recall = get_recall(e, g)\n",
    "        r.append(recall)\n",
    "        fscore = get_fscore(precision, recall)\n",
    "        f.append(fscore)\n",
    "\n",
    "    metrics = [statistics.mean(p), statistics.mean(r), statistics.mean(f)]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_df(results_df):\n",
    "    raw_gold = results_df['gold tags'].to_list()\n",
    "    metrics_raw = {'metrics': ['precision', 'recall', 'f-score']}\n",
    "    for col in results_df.columns[1:]:\n",
    "        raw_extracted = results_df[col].to_list()\n",
    "        metrics_raw[col] = get_results(raw_extracted, raw_gold)\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_raw)\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>textrank</th>\n",
       "      <th>rake trigrams</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.029319</td>\n",
       "      <td>0.030135</td>\n",
       "      <td>0.086667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.235374</td>\n",
       "      <td>0.438364</td>\n",
       "      <td>0.146867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f-score</td>\n",
       "      <td>0.047984</td>\n",
       "      <td>0.053953</td>\n",
       "      <td>0.106573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     metrics  textrank  rake trigrams    tf-idf\n",
       "0  precision  0.029319       0.030135  0.086667\n",
       "1     recall  0.235374       0.438364  0.146867\n",
       "2    f-score  0.047984       0.053953  0.106573"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_raw_df = create_metrics_df(results_raw_df)\n",
    "\n",
    "metrics_raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__!rake trigrams!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw_df.to_csv('metrics_raw.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>textrank</th>\n",
       "      <th>rake trigrams</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.035002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.185428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f-score</td>\n",
       "      <td>0.053752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     metrics  textrank  rake trigrams    tf-idf\n",
       "0  precision  0.035002            0.0  0.133333\n",
       "1     recall  0.185428            0.0  0.220210\n",
       "2    f-score  0.053752            0.0  0.162375"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_lemmas_df = create_metrics_df(results_lemmas_df)\n",
    "\n",
    "metrics_lemmas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__!tf_idf!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_lemmas_df.to_csv('metrics_lemmas.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas with POS-tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>textrank</th>\n",
       "      <th>rake trigrams</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.044527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.260736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f-score</td>\n",
       "      <td>0.070713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     metrics  textrank  rake trigrams    tf-idf\n",
       "0  precision  0.044527            0.0  0.086667\n",
       "1     recall  0.260736            0.0  0.141147\n",
       "2    f-score  0.070713            0.0  0.105094"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_lemmas_pos_df = create_metrics_df(results_lemmas_pos_df)\n",
    "\n",
    "metrics_lemmas_pos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__!textrank!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_lemmas_pos_df.to_csv('metrics_lemmas_pos.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas with syntactic patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>textrank</th>\n",
       "      <th>rake trigrams</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.068348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.260736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f-score</td>\n",
       "      <td>0.102185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     metrics  textrank  rake trigrams    tf-idf\n",
       "0  precision  0.068348            0.0  0.128266\n",
       "1     recall  0.260736            0.0  0.141147\n",
       "2    f-score  0.102185            0.0  0.131035"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_lemmas_pos_patt_df = create_metrics_df(results_lemmas_pos_patt_df)\n",
    "\n",
    "metrics_lemmas_pos_patt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Results are not so good\n",
    "* RAKE trigrams are the best on the raw texts. But I think it is because of a large number of the extracted keywords for each text.\n",
    "* Quality of TextRank is overall good.\n",
    "* Syntactic patterns give better quality when working with tags.\n",
    "* Needed keywords are often parts of collocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO:__\n",
    "* Fix omissions in the data after parsing (_propn after TextRank and TF-IDF).\n",
    "* Morphological and syntactical parsing of the gold standard?\n",
    "* There is a little problem with markup of the POS-tags because abbreviations in our case have tags PROPN and NOUN but parser put PROPN tag on all abbreviations. So by metrics the results must be better.\n",
    "* Threshold for the output of models.\n",
    "* Lemmas comparison for the raw texts.\n",
    "* Reward for partial matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On the Habr the authors are those who tag their articles. So the unite algorithm to put tags does not exist.\n",
    "* From the syntactical point of view users prefer different NP`s. They do not prefer to put specific names in the tags but they mention a lot of abbreviations and common domain words. From the semantic point of view the also prefer put abstract common domain expressions that can not refer to the specific expressions in the text.\n",
    "* RAKE extractor can not work with lemmatized texts. `summa` has problems with tagged words.\n",
    "* From metrics results we can see that models solve keyword extraction task not so good. \n",
    "* I think that by quality the work of models is quite good for machine keyword extraction because they catched a lot of informative words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
