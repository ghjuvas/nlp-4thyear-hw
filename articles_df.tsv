	link	original tags	text	new tags	gold tags	keywords length	lemmatized gold tags	lemmatized pos gold tags	patterns gold tags
0	https://habr.com/ru/company/just_ai/news/t/698860/	искусственный интеллект, голосовые интерфейсы, голосовые ассистенты, разговорный ии, nlp (natural language processing), синтез речи, чат-бот, конференция, tts, voice assistant	"2 декабря в Москве в онлайн- и офлайн-формате состоится Conversations – ежегодная конференция по разговорному AI для разработчиков и бизнеса. Про NLP-сервисы, диалоговые платформы и фреймворки, синтез и распознавание речи, UX и проектирование разговорных интерфейсов, генеративные модели и не только расскажут KODE, MTS AI, Альфа-Банк, Сбер, Yandex Cloud, DeepPavlov и другие эксперты. В нашем анонсе – особо интригующие спойлеры и промокод на скидку.

Участников конференции ждет 2 потока докладов – Business и Technology. С полной программой можно познакомиться на сайте Conversations, ну а мы предлагаем погрузиться именно в Technology Track.
Секция «Good morning, ML!»  
Что делать, если исходящий звонок колл-центра попадает на голосовую почту или умного ассистента? О технологиях ML для создания модели идентификации голосовой почты и виртуального помощника расскажет Артем Бондарь, Voximplant.Мурат Апишев из Just AI раскроет детали работы над индустриальной NLP-платформой: как организовали управление NLP-сервисами и моделями, стандартизировали встраивание собственных и open-source решений и выучили свой управляемый парафраз для помощи пользователям.Как сделать модель, которая понимает всех — от тёти Сары до фрау Заурих, и экономично использует вычислительные ресурсы? Ответы готовы у Антона Ермилова, Yandex Cloud. Мария Тихонова, SberDevices, проведет экскурсию по «Зоопарку генеративных моделей 2022» и расскажет про все современные инструменты работы с текстом на основе генеративных моделей.
Секция «Bot's developing: от сценария к личности»
Никита Блинков из VK расскажет о подходах к измерению и развитию умности голосового помощника: выделение крупных классов запросов, улучшение качества ответов по классам и группам внутри них, а также автоматический набор входных фраз для отдельных интентов.Что важнее при разработке личности ассистента – ML, психология или сценаристика? Ангелина Большина и Галина Прохорова из MTS AI покажут многопрофильный подход к разработке личности и раскроют инсайты из своего исследования (например, почему технические возможности тонкой настройки личности в функционале, который основан не на правилах, а на ML-инструментах, ограничены?). Классические блок-схемы устарели? Почему древовидные схемы не подходят для проектирования сложных разговорных сценариев? Об альтернативах – AirTable, Fable и эволюционном подходе к проектированию блок-схем с помощью языка Дракон поговорим с Кириллом Богатовым, KODE.
Секция «Set tool. Инструменты и практики»  
Как объединить высокую управляемость сценарного подхода в комбинации с использованием генеративных языковых моделей? Об управлении через автоматическое построение диалоговых графов и контроль диалога с помощью PALs и Grounding Knowledge расскажут Денис Кузнецов и Данила Корнев, DeepPavlov.Андрей Татаринов из AGIMA AI поделится инсайтами работы с фреймворком RASA и расскажет, как сделать бота с 1500+ интентами и иерархическим классификатором.Про опыт трансформации из legacy-систем в единое платформенное решение расскажут спикеры Альфа-Банка Наталья Балыбердина и Станислав Милых. Ребята научат, как создать целевую инфраструктуру для ботов в банке и какую аналитику учесть при планировании виртуального помощника в чате.
Больше спикеров и тем – на сайте Conversations!  Присоединяйтесь! Участвовать можно и офлайн, и онлайн. 
Промокод на скидку 10% для читателей Хабра: CNVS22_UnR, билеты здесь Узнать о том, как это было в прошлом году, можете по ссылке.

"	Conversations, конференция, NLP, диалоговые платформы, синтез речи, распознавание речи, UX, проектирование, генеративные модели, KODE, MTS AI, Альфа-Банк, Сбер, DeepPavlov, Voximplant, Just AI, Yandex Cloud, SberDevices, VK, AGIMA AI, промокод	conversations, deeppavlov, sberdevices, yandex cloud, just ai, диалоговые платформы, искусственный интеллект, voximplant, проектирование, альфа-банк, vk, kode, синтез речи, mts ai, agima ai, распознавание речи, ux, сбер, конференция, разговорный ии, nlp, генеративные модели, промокод	23	conversations, deeppavlov, sberdevices, yandex cloud, just ai, диалоговый платформа, искусственный интеллект, voximplant, проектирование, альфа-банк, vk, kode, синтез речь, mts ai, agima ai, распознавание речь, ux, сбер, конференция, разговорный ии, nlp, генеративный модель, промокод	conversations_PROPN, deeppavlov_PROPN, sberdevices_PROPN, yandex_PROPN cloud_NOUN, just_ADP ai_NOUN, диалоговый_ADJ платформа_NOUN, искусственный_ADJ интеллект_NOUN, voximplant_PROPN, проектирование_NOUN, альфа-банк_PROPN, vk_PROPN, kode_PROPN, синтез_NOUN речь_NOUN, mts_PROPN ai_PROPN, agima_PROPN ai_NOUN, распознавание_NOUN речь_NOUN, ux_NOUN, сбер_PROPN, конференция_NOUN, разговорный_ADJ ии_NOUN, nlp_NOUN, генеративный_ADJ модель_NOUN, промокод_NOUN	conversations_PROPN, deeppavlov_PROPN, sberdevices_PROPN, диалоговый_ADJ платформа_NOUN, искусственный_ADJ интеллект_NOUN, voximplant_PROPN, проектирование_NOUN, альфа-банк_PROPN, vk_PROPN, kode_PROPN, синтез_NOUN речь_NOUN, распознавание_NOUN речь_NOUN, ux_NOUN, сбер_PROPN, конференция_NOUN, разговорный_ADJ ии_NOUN, nlp_NOUN, генеративный_ADJ модель_NOUN, промокод_NOUN
1	https://habr.com/ru/company/first/blog/699380/	open source, ai, ml, инструменты разработчика, открытое по	"
От создания дипфейков до обработки естественного языка — опенсорс-инструменты всегда находятся на переднем крае разработок с искусственным интеллектом и машинным обучением. Концепция открытого исходного кода и приложения для совместной работы упрощают обмен данными в командах и делают производственный цикл значительно короче.
Неудивительно, что open source с каждым годом завоевывает все большую популярность и начинает преобладать даже в корпоративном секторе, где традиционно доминировало проприетарное ПО. Согласно опросу, проведенному Red Hat среди почти 1300 ИТ-руководителей крупнейших компаний мира, свыше 80% предприятий планируют увеличить использование enterprise-технологий с открытым исходным кодом в ближайшие два года.
В этой статье рассмотрим 15 проектов с открытым исходным кодом, которые на наших глазах меняют мир искусственного интеллекта и машинного обучения. Некоторые из них — просто программные пакеты на основе AI/ML алгоритмов, другие — полнофункциональные фреймворки или платформы для машинного обучения. Но каждый из представленных ниже инструментов достоин внимания разработчика, интересующегося перспективными технологиями.
TensorFlow
Источник: github.com/tensorflow
Категория: фреймворк, глубокое обучение (DL), обработка естественного языка (NLP), компьютерное зрение (CV), обработка звука (AP)Лицензия: Apache License 2.0
Для любого, кто использует программное обеспечение с AI/ML, фреймворк TensorFlow от Google не нуждается в представлении. Он начался как внутренний проект команды Google Brain в 2011 году, основанный на нейронных сетях глубокого обучения, а с 2015 года начал развиваться в направлении открытого исходного кода.
Одно из главных преимуществ TensorFlow — его обучающая экосистема. Те, кто только начинают разработку опенсорнных AI/ML продуктов, легко найдут бесплатные учебные пособия, исчерпывающие курсы и сертификаты в дополнение к собственной подробной документации. Еще одно явное преимущество TensorFlow — почти абсолютная гибкость, ведь фреймворк можно использовать на любом языке и в любой производственной среде.
Особенности:
Поддержка нескольких языков, включая JavaScript.Интуитивно понятные высокоуровневые API (например, Keras) для простого создания и обучения моделей машинного обучения.Независимые от платформы ML-процессы, которые можно развертывать локально — в облаке, браузере или на устройстве.Приложение TensorFlow Lite для мобильных приложений и встроенных устройств или IoT.Перекрестная совместимость между моделями AI/ML, которые были обучены на разных версиях TensorFlow.Широкий спектр приложений, включая прогнозный анализ, классификацию объектов и диалоговый ИИ.
PyTorch
Источник: github.com/pytorch
Категория: фреймворк, глубокое обучение (DL), обработка естественного языка (NLP), компьютерное зрение (CV), обработка звука (AP)Лицензия: BSD License
Фреймворк PyTorch совершенствует базовую структуру ML-библиотеки Torch, созданной на языке Lua. Исследовательская лаборатория искусственного интеллекта FAIR запустила PyTorch в качестве интерфейса на основе Python для разработки AI/ML приложений под лицензией с открытым исходным кодом в 2016 году. Сегодня PyTorch превратился в богатую экосистему, которая предоставляет вам все инструменты, необходимые для ускорения разработки ИИ, — от исследований до производства.
Самым большим УТП PyTorch, вероятно, является его доступность в популярных облачных сервисах, типа Alibaba Cloud. Это позволяет быстро загрузить библиотеку программного обеспечения из соответствующего магазина приложений и приступить к работе, не выходя из текущей облачной среды разработки.
Особенности:
Готовая к производству среда на базе TorchServe для быстрого развертывания моделей.Распределенная внутренняя архитектура для оптимизации производительности.Алгоритмы компьютерного зрения и обработки естественного языка.Пайплайн основан на Python, но доступен экспорт в проекты на C++ с помощью дополнительного инструмента TorchScript. Поддерживается всеми основными общедоступными облаками для гибкой разработки, включая Alibaba Cloud, Amazon Web Services, Google Cloud Platform и Microsoft Azure.Поддерживается ведущими производителями чипов, включая ARM, Nvidia, Qualcomm и Intel.Сквозной рабочий процесс — от Python до iOS/Android для разработки мобильных приложений.Возможен собственный экспорт из открытой библиотеки Open Neural Network Exchange (ONNX).
H2O.ai
Источник: h2o.ai
Категория: ML-платформа, глубокое обучение (DL), Cloud SaaSЛицензия: Apache License 2.0
Компания H2O с момента основания в 2012 году находится в авангарде инноваций в области AI и ML с открытым исходным кодом. Она работает над созданием крупномасштабных продуктов на базе ИИ с такими технологическими гигантами, как NVIDIA, IBM, Intel и Google. Платформа основана на Kubernetes, поэтому ее можно запустить в любом облаке или в локальной инфраструктуре.
Главное детище H2O — собственная облачная масштабируемая платформа для глубокого обучения и предиктивной аналитики. H2O AI Cloud позволяет компаниям использовать все преимущества гибридных облаков — подготавливать, моделировать, эксплуатировать, разрабатывать и использовать в централизованной среде, привлекая к сотрудничеству сторонних подрядчиков. 
Особенности:
Интеграция с Hadoop и Spark для моделирования ИИ на основе больших данных.Большая библиотека ML-алгоритмов, включая контролируемое и неконтролируемое обучение.Встроенный механизм прогнозирования схем входящих наборов данных.Поддержка приема данных из нескольких источников в различных форматах.ИИ-помощник, помогающий пользователям со слабой технической базой подготавливать данные, задавать параметры и выбирать алгоритмы для решения конкретных бизнес-задач.Простая в использовании UI-навигация через Flow.
Blender
Источник: blender.org
Категория: приложение, 3D визуализацияЛицензия: GNU General Public License (GPL)
Мечтаете получить «Оскар» за 3D-анимацию и эффекты? Тогда вам точно стоит попробовать Blender. Это лучший способ преобразовать статичные трехмерные модели в богато визуализированные сцены видеоролика или полнометражки. 
Хотя многие считают его инструментом для создателей фильмов и аниматоров, Blender также является отличным примером прикладного применения искусственного интеллекта. Богатый интерфейс и многочисленные плагины позволяют любому желающему создавать сложную моушн-графику или кинематографические перспективы. Нужно добавить немного собственного творчества и комитет по номинациям на «Оскар» скоро свяжется с вами (ИИ в соавторах можно не указывать).
Особенности:
Поддерживает полный пайплайн (цикл разработки) 2D и 3D моделей — моделирование, оснастку, анимацию, симуляцию, рендеринг, компоновку и отслеживание движений, редактирование видео.Три встроенных движка: Workbench в окне просмотра (viewport), Cycles Render с алгоритмом, основанным на трассировке света и Eevee, который выполняет PBR-рендеринг (Physically based rendering) в реальном времени.Включает готовую к производству камеру и трекинг объектов, что позволяет импортировать необработанные кадры, отслеживать кадры, маскировать области и видеть движения камеры в 3D-сцене в реальном времени.Настраиваемая панель пользователя.
DALL·E
Источник: openai.com
Категория: обработка естественного языка (NLP), генератор изображений, нейросетьЛицензия: Modified MIT License (GPL)
Один из самых простых способов понять, как «думают» модели машинного обучения, — начать вставлять слова в DALL·E 2, очень большую открытую ИИ-модель, обученную на изображениях и текстовых описаниях из интернета. Вводим слово в нейросеть и получаем на выходе подходящее по смыслу реалистичное изображение. Это отчасти игра, а отчасти портал в «разум» ИИ-алгоритма.
На сегодняшний день DALL·E 2 реализована в закрытой версии нового приложения Microsoft Designer, а также в дизайнерских приложениях CALA и Mixtiles от OpenAI, которые используют ее API. Лично «познакомиться» с нейросетью можно в облачном сервисе DALL-E Playground с различными моделями генерации (например, DALL·E Mini и DALL·E Mega). Инструмент пока бесплатный, но есть ограничение — 1 запрос в 10 секунд.
Особенности:
Мощный творческий инструмент — DALL·E может стать источником «быстрого» вдохновения для целого ряда творческих профессий, включая дизайнеров, художников и режиссеров.Функция редактирования полученных изображений (Edit), которая позволяет дополнять описание на естественном языке для получения более реалистичного результата.Функция Variations позволяет выбирать источники для генерации — пользовательские изображения или те, что нашла сама DALL·E.Функция My Collection позволяет пользователям сохранять сгенерированные изображения прямо на платформе DALL·E.Защита от неправомерного использования — фильтры контента будут блокировать изображения, нарушающие нормы морали или политику конфиденциальности личной информации.
YOLOv7
YOLOv7 + корпоративная платформа для развертывания компьютерного зрения Viso Suite / Источник: viso.ai
Категория: алгоритм, нейронная сеть, обнаружение объектов (OD), компьютерное зрение (CV)Лицензия: GNU General Public License (GPL)
Обнаружение объектов в реальном времени или поиск объектов на изображениях — сложная область, даже для искусственного интеллекта. Но подобная функциональность критически значима для программного обеспечения таких производственных областей, как робототехника, беспилотные автомобили и вспомогательные устройства, которым необходимо собирать и передавать точную информацию об окружающей среде. 
YOLOv7 (от You Only Look Once) — один из самых быстрых и точных инструментов обнаружения объектов с открытым исходным кодом. Для него требуется в несколько раз более дешевое оборудование, чем для других нейронных сетей, и его можно обучать намного быстрее на небольших наборах данных без каких-либо предварительно обученных весов. 
Особенности:
Самая высокая точность (56,8% AP) среди всех известных детекторов объектов реального времени с 30 FPS и выше на GPU V100.Собственная ноу-код платформа Viso Suite для развертывания моделей компьютерного зрения и создания пользовательских приложений. Для обучения используется только набор данных MS COCO.Архитектура E-ELAN (Extended Efficient Layer Aggregation Network) позволяет улучшить способность сети к обучению, не разрушая исходный градиентный путь.Метод составного масштабирования моделей.
DeepFaceLab
Источник: deepfakevfv.com
Категория: глубокое обучение (DL), обработка видео, дипфейкиЛицензия: GNU General Public License (GPL)
Дипфейки — видео и изображения, созданные, измененные или синтезированные с помощью технологий глубокого обучения. Наиболее распространенный пример использования дипфейков — подмена лиц знаменитостей или политиков для создания смешных видео. Именно так появился знаменитый ролик, где актёр Джордан Пил, «поменялся местами» с Баракой Обамой. 
DeepFaceLab — ПО для дипфейков с открытым исходным кодом, работающее на Python. Более 90% дипфейк-видео, которые можно видеть в Интернете, создаются с помощью именно этой технологии. На DeepFaceLab основан контент большинства популярных ютуб-каналов, посвященных дипфейкам, включая Corridor Crew, Collider Extras и VFXChris Ume.
Помимо замены одного лица на другое в дипфейках, DeepFaceLab можно использовать и для более практичных вещей. Например, для удаления морщин и других возрастных признаков возраста на видео. 
Особенности:
Взаимозаменяемость модулей.Собственная высокоуровневая инфраструктура глубокого обучения Leras (от Lighter Keras), построенная на чистом TensorFlow.Возможность гибкой настройки разных аспектов дипфейк-конвейера с помощью инструментов командной строки.Встроенный редактор ручной сегментации лиц XSeg.Средства повышения производительности: поддержка нескольких графических процессоров, обучение с половинной точностью, использование закрепленной памяти CUDA, использование нескольких потоков данных.Реализация DeepFaceLive позволяет вычислять модели DeepFaceLab со скоростью видеопотока (около 30 кадров в секунду) и создавать дипфейки в режиме реального времени, например, для видеозвонков.
MindsDB
Визуализация прогностических данных, собранных MindsDB / Источник: deepfakevfv.com
Категория: SQL-сервер, базы данныхЛицензия: GNU General Public License (GPL)
Обычный способ взаимодействия с ИИ — хранить данные в БД, а затем извлекать их для отправки в отдельный алгоритм машинного обучения. SQL-сервер MindsDB интегрирует алгоритмы машинного обучения сразу в базу данных. Анализ данных в месте хранения позволяет значительно ускорить рабочие процессы машинного обучения.
Особенности:
Бесшовная интеграция с большинством баз данных. Использование SQL-запросов для всех манипуляций с моделями ML. Использование мощностей графического процессора для повышения скорости обучения моделей без понижения производительности базы данных. Визуальные инструменты, позволяющие исследовать производительность модели.Встроенная библиотека Lightwood AutoML, позволяющая автоматизировать построение и обучение моделей на пользовательских данных.
Image Super-Resolution (ISR)
Изображение с низким разрешением (слева), вывод ISR (в центре), бикубическая интерполяция (справа) / Источник: idealo.github.io
Категория: библиотека, обработка изображений, компьютерное зрение (CV)Лицензия: Apache License 2.0
Известно, что чем выше детализация фотографий, тем лучше их качество. ИИ-библиотека ISR («Изображение со сверхвысоким разрешением») помогает добавить фото еще больше деталей, увеличив разрешение изображения. 
Этот инструмент с открытым исходным кодом использует ML-модель, которую можно научить «угадывать» детали изображения с низким разрешением. С хорошим тренировочным набором модель может воспроизводить точные детали и повышать четкость изображения, что важно для различных задач компьютерного зрения.
Особенности:
Алгоритмы реализованы на платформе фреймворка PyTorch.ISR совместим с последними версиями Python.Для обучения и прогнозирования используются скрипты Docker и блокноты Google Colab.Готовые скрипты для облегчения обучения в облаке на AWS и Nvidia-docker с помощью всего нескольких команд.
DeepPavlov
Источник: ipavlov.ai
Категория: фреймворк, глубокое обучение (DL), обработка естественного языка (NLP), чат-ботЛицензия: Apache License 2.0
Многие предприятия и крупные корпорации заменяют передовые линии обслуживания клиентов чат-ботами, а это означает, что машины учатся поддерживать разговор. Платформа диалогового ИИ DeepPavlov объединяет основные инструменты машинного обучения, такие как TensorFlow, Keras и PyTorch, для создания чат-ботов и виртуальных помощников.
DeepPavlov снабжен комплексными и гибкими инструментами, которые позволяют разработчикам и исследователям NLP создавать готовые к работе разговорные навыки и сложных помощников для разговоров с несколькими навыками. Фреймворк позволяет создавать промышленные решения с многопрофильной интеграцией через сервисы API.
Особенности:
Запуск предварительно обученных или собственных NLP-компонентов и разговорных навыков.Фреймворк использует BERT и другие современные модели глубокого обучения для решения задач классификации, NER (распознавания именованных сущностей), вопросов-ответов и других NLP-задач. Модели DeepPavlov упакованы в простые для развертывания контейнеры, размещенные на Nvidia NGC и Docker Hu.Технология диалогового искусственного интеллекта может помочь ускорить NLP-приложения до 20 раз.
Mycroft.ai
Источник: github.com/MycroftAI
Категория: голосовой помощник, обработка естественного языка (NLP)Лицензия: Apache License 2.0
Голосовой помощник с открытым исходным кодом «Майкрофт» предлагает доступную альтернативу коммерческим решениям от лидеров индустрии, таким как Alexa или Siri, имеющим очевидные проблемы с майнингом пользовательских данных. Существует большое сообщество пользователей, разработчиков и переводчиков, которые постоянно улучшают алгоритмы ИИ и обеспечивают поддержку решения.
Особенности:
Можно запустить в любой экосистеме.Пользовательский интерфейс на естественном языке (LUI/NLUI).Релизы доступны для Android, Linux и Docker, а также для macOS и Windows через виртуальную машину VirtualBox.Модульная архитектура со сменными внутренними компонентами.Преобразование речи в текст совместно с Mozilla Common Voice Project и программным обеспечением DeepSpeech.Парсинг поисковых интентов через преобразования естественного языка в машиночитаемые структуры данных.Преобразование текста в речь на основе системы синтеза речи Festival Lite.
OpenNN
Источник: opennn.net
Категория: библиотека, нейронные сети (NN)Лицензия: GNU General Public License (GPL)
OpenNN (Open Neural Networks Library) — программная библиотека искусственного интеллекта с открытым исходным кодом для реализации нейронных сетей и машинного обучения. Основная специализация — прогнозная и отраслевая аналитика.
На OpenNN можно создавать приложения для сегментации клиентов в маркетинге, ранней диагностики в здравоохранении, профилактического обслуживания оборудования и многого другого.
Особенности:
Библиотека создана на основе C++.Регрессионный анализ для моделирования результатов машинного обучения.Классификация данных для назначения определенных шаблонов.Прогнозирование на основе исторических наборов данных.Отображение ассоциации между двумя коррелированными переменными.ML-инструмент Neural Designer («нейронный дизайнер») для упрощения процесса построения нейронных сетей.Хорошая документированность.
OpenCV
Обнаружение объектов с помощью DNN-модуля (Deep Neural Network) OpenCV / Источник: wikipedia.org
Категория: библиотека, компьютерное зрение (CV)Лицензия: BSD / Apache License 2.0
Библиотека OpenCV с открытым исходным кодом может стать одним из самых эффективных инструментов для изучения технологии машинного зрения. Он включает в себя множество популярных алгоритмов для идентификации объектов на цифровых изображениях, а также ряд специализированных процедур, например, возможность определять и считывать номерные знаки автомобилей.
Особенности:
OpenCV изначально разрабатывался на C++, а позже были добавлены привязки Python и Java. Библиотека работает в различных операционных системах, включая Windows, Linux, OSx, FreeBSD, Net BSD, Open BSD и т. д.OpenCV включает 6 функциональных модулей: Video (оценка движения, вычитание фона, отслеживание объектов), Video I/O (захват видео), calib3d (калибровка камеры), features2d (обнаружение и описание функций), Objdetect (обнаружение объектов и экземпляров предопределенных классов — лица, глаза, лица, люди, автомобили и т. д.), Highgui (пользовательский интерфейс).
FauxPilot
Источник: github.com/moyix/fauxpilot
Категория: ИИ-помощник, подсказки и автозавершение кодаЛицензия: Modified MIT License (GPL)
Программисты, которым нужна небольшая помощь в написании кода, могут получить ее в FauxPilot. Система обучается на существующем производственном коде, чтобы делать структурированные комментарии и предложения по автозаполнению. 
Проект был вдохновлен GitHub Copilot, но, в отличие от оригинала, FauxPilot позволяет выбирать репозитории, которые будут использованы для обучения. Этот дополнительный уровень контроля позволяет с большей вероятностью выбрать чистые и заслуживающие доверия учебные источники.
Особенности:
FauxPilot позволяет обойти проблемы с лицензированием и телеметрией, возникающие при отправке GitHub Copilot данных в Microsoft.FauxPilot, как и Copilot основан на системе преобразования естественного языка в код OpenAI Codex (GPT-3), обученной на миллиардах строк открытого исходного кода из проектов GitHub.Модели FauxPilot используют для обучения ИИ-модель CodeGen от Salesforce.
Robocode
Источник: sourceforge.net
Категория: игры, робототехника, обучение программированию, беспилотные транспортные средстваЛицензия: Eclipse Public License (EPL)
Работа с ИИ необязательно должна быть скучным занятием. Есть способ превратить создание алгоритмов на основе искусственного интеллекта в драматичную схватку на выживание в стиле «Голодных игр». 
«Робокод» — игра на основе Java, где вашему заранее запрограммированному робо-танку придется сражаться против других машин в битве за господство над ареной. Это веселое времяпрепровождение, которое может быть даже полезно для тестирования новых стратегий управления автономными транспортными средствами.
Кстати. Источником вдохновения для Robocode послужила игра для программистов Robot Battle, написанная Брэдом Шиком в 1994 году. Robot Battle, в свою очередь, была вдохновлена RobotWar — игрой Apple II+ начала 1980-х. 
Особенности:
Полная среда разработки, которая поставляется с установщиком, встроенным редактором роботов и компилятором Java.Помимо Java можно программировать на других языках, например, таких, как Kotlin и Scala.Возможность выбора версии Java — Java Runtime Environment (JRE) или Java Development Kit (JDK).Поддержка разработки роботов с использованием внешних IDE.
Игровая концепция Robocode отлично подходит для применения в учебных заведениях.
Заключение
Пятнадцать опенсорс-технологий, которые мы кратко описали выше, отвечают профессиональным требованиям к качественному AI/ML инструменту. Они поддерживаются большими сообществами разработчиков, получают регулярные усовершенствования и итерации, а также прошли проверку использованием в реальных проектах. Эти продукты объединяют коллективный разум мирового сообщества разработчиков и возможности самых передовых исследовательских лабораторий в мире.
Материал подготовлен на основе статьи Peter Wayner с использованием открытых источников и авторскими дополнениями.  
"	технологии, опенсорс, машинное обучение, глубокое обучение, искусственный интеллект, AI, ML, DL, NLP, CV, визуализация, нейросеть, нейронная сеть, обработка видео, обработка звука, дипфейки, генерация изображений, обнаружение объектов, базы данных, обработка естественного языка, голосовой помощник, автозавершение кода, робототехника, обучение программированию, приложение, игры, библиотека, фреймворк	технологии, нейронная сеть, приложение, ai, глубокое обучение, дипфейки, ml, автозавершение кода, open source, генератор изображений, обработка звука, базы данных, обнаружение объектов, искусственный интеллект, фреймворк, cv, обработка видео, робототехника, нейросеть, dl, голосовой помощник, визуализация, обучение программированию, игры, обработка естественного языка, nlp, библиотека, опенсорс	28	технология, нейронный сеть, приложение, ai, глубокий обучение, дипфейк, ml, автозавершение код, open source, генератор изображение, обработка звук, база данные, обнаружение объект, искусственный интеллект, фреймворк, cv, обработка видео, робототехника, нейросеть, dl, голосовой помощник, визуализация, обучение программирование, игра, обработка естественный язык, nlp, библиотека, опенсорс	технология_NOUN, нейронный_ADJ сеть_NOUN, приложение_NOUN, ai_NOUN, глубокий_ADJ обучение_NOUN, дипфейк_NOUN, ml_NOUN, автозавершение_NOUN код_NOUN, open_ADJ source_NOUN, генератор_NOUN изображение_NOUN, обработка_NOUN звук_NOUN, база_NOUN данные_NOUN, обнаружение_NOUN объект_NOUN, искусственный_ADJ интеллект_NOUN, фреймворк_NOUN, cv_NOUN, обработка_NOUN видео_NOUN, робототехника_NOUN, нейросеть_NOUN, dl_NOUN, голосовой_ADJ помощник_NOUN, визуализация_NOUN, обучение_NOUN программирование_NOUN, игра_NOUN, обработка_NOUN естественный_ADJ язык_NOUN, nlp_NOUN, библиотека_NOUN, опенсорс_NOUN	технология_NOUN, нейронный_ADJ сеть_NOUN, приложение_NOUN, ai_NOUN, глубокий_ADJ обучение_NOUN, дипфейк_NOUN, ml_NOUN, автозавершение_NOUN код_NOUN, open_ADJ source_NOUN, генератор_NOUN изображение_NOUN, обработка_NOUN звук_NOUN, база_NOUN данные_NOUN, обнаружение_NOUN объект_NOUN, искусственный_ADJ интеллект_NOUN, фреймворк_NOUN, cv_NOUN, обработка_NOUN видео_NOUN, робототехника_NOUN, нейросеть_NOUN, dl_NOUN, голосовой_ADJ помощник_NOUN, визуализация_NOUN, обучение_NOUN программирование_NOUN, игра_NOUN, nlp_NOUN, библиотека_NOUN, опенсорс_NOUN
2	https://habr.com/ru/company/alfa/blog/698660/	хакатон, граф, машинное обучение, нейросети, соцсети	"В сентябре 2022 проходил хакатон «Машинное обучение на графах» от компании ВК на платформе «Цифровой прорыв». В хакатоне участвовала команда Лаборатории машинного обучения Альфа-Банка: Александр Сенин, Георгий Смирнов и Валерий Смирнов.
Мы заняли 1 место в хакатоне, далее подробно расскажем, как нам удалось победить.
Контекст
В хакатоне допускалось участие команд до 5 человек в каждой. Основная масса участников — студенты вузов МГУ, МФТИ, ИТМО, МИРЭА. Наша команда не была исключением, мы студенты последних курсов ВМК и мехмата МГУ. 
Особенности хакатона:
В отличие от привычных долгих соревнований по машинному обучению, хакатон длился всего 43 часа. Ограничение по времени необходимо было учитывать при построении решения.Хакатон был полностью очным. Все 43 часа требовалось находиться на площадке в офисе ВК. Провести две ночи в офисе — необычный опыт.Финальное ранжирование команд проводилось не только по скору в лидерборде, но и с учетом оценок жюри. 
Склонность к благотворительности
Задача заключалась в построении модели склонности пользователей социальной сети ВК к благотворительности. С точки зрения машинного обучения — это старая добрая задача бинарной классификации. Для построения модели были выданы данные реальных пользователей в анонимизированном виде. 
Данные органично распались на 3 домена:
Табличные данные — больше 1000 признаков для каждого из 160 000 пользователей.Последовательности пользовательских состояний — последовательности хэшей средней длины 140, известные для 77% пользователей.Признаковые описания топа друзей, известные для 81% пользователей.
Метрика качества — всем привычный ROC AUC. Локально оценивали модели по кросс-валидации.
Обсудим каждый источник данных отдельно. 
Табличные данные
Организаторы соревнования преисполнились в вопросе тщательной анонимизации данных — среди 1000 признаков было лишь два столбца с незашифрованными названиями: идентификатор клиента и дата. 
Однако, в названиях остальных признаков были оставлены подсказки. Примеры названий: u0=11, u8=6, i1056. 
CLIENT_IDRETRO_DTu0=10u1=0...i1056i450100010005.06.210.00.0...0.05.0100012106.06.211.07.3...0.09.0
Напрашивалось разбить признаки на группы, например, все признаки с префиксом u0. После разбивки нагенерировали новых признаков, применив агрегаты внутри одной группы признаков: сумма группы признаков, среднее и стандартное отклонение, минимум и максимум и т.д. После небольшого EDA добавили других признаков, а также удалили откровенно мусорные признаки, например, константные. 
Не обошлось и без ликов. Наверное, самый популярный в соревнованиях по машинному обучению — это лик в идентификаторе объекта. Добавление признака идентификатора клиента в числовом виде улучшало качество модели на 2 пункта ROC AUC. Скорее всего, идентификатором выступал номер строки в таблице, причем в момент его создания строки не были упорядочены абсолютно случайно. Однако, формат хакатона подразумевает честное рабочее решение без читов с ликами, поэтому от использования такого признака пришлось отказаться.
Последовательные данные
Последовательности пользовательских состояний были представлены в виде списка хэшей, например ['e84b0', '9a767', '0be67', ...]. Здесь можно увидеть еще одну подсказку. В данном случае хэши представлены в строковом виде, а значит имеем последовательность строк. Последовательность строк можно воспринимать как последовательность слов и смотреть на задачу в парадигме обработки текстов на естественном языке (NLP). 
CLIENT_IDSEQUENCE1000100['e84b0f', '471b8', 'e8f4a', …] 1000121['ecc81', 'eb27b', '16c09', …]1000131['9a767', 'e84b0', '0be67', …]1000132['b496d', 'e8f4a', '16c09', …]100013['9704a', 'e84b0', 'e84b0', …]
В этот момент многие команды бросились сразу учить сложные нейросетевые модели для построения эмбеддингов состояний, а по ним и последовательностей. Вообще говоря, мы в Лаборатории машинного обучения занимаемся именно обучением сложных нейросетевых моделей (от модели дефолта по последовательностям транзакций до чат-бота), но в этой ситуации выбрали другую тактику. Решили сперва построить простое MVP решение в стиле классического ML, а уже затем улучшать его. 
В NLP задачах существует популярный бейзлайн с использованием TF-IDF представлений, который часто бывает непросто побить. Применили в нашей задаче TF-IDF, получили 34 000 признаков для каждой последовательности. Понизили размерность через сингулярное разложение до 512 признаков. Это преобразование можно воспринимать как построение эмбеддинга пользователя по последовательности состояний. Такой бейзлайн улучшил итоговое качество на 2 пункта ROC AUC.

Данные друзей
По 81% пользователей были даны признаки их друзей, в среднем, по 76 друзей на пользователя. Признаки здесь те же самые, что и в признаковом описании самого пользователя — вновь признаки с префиксами вида u0=<число>, i<число> и т.д. 
Кроме этих признаков, был ещё один — идентификатор друга. Однако, множество идентификаторов друзей не пересекалось со множеством идентификаторов пользователей. Связать ребрами пользователей только по идентификаторам не получилось бы. Не было и данных по друзьям друзей. Организаторы объяснили это так: если брать данные по всем друзьям в окрестности больше 1, то пришлось бы выгружать весь граф ВК. К тому же, не был известен таргет у друзей.
CLIENT_IDRETRO_DTu0=10u1=0...i1056FRIEND_ID100010004.04.211.04.2...0.0100010010100010001.04.211.01.3...0.0100010025
Сам по себе идентификатор друга отличался по структуре от идентификатора пользователя: он получался конкатенацией идентификатора пользователя с некоторым двузначным числом. Участники хакатона из других команд предположили, что в этом двузначном числе есть информация о расстоянии от пользователя до его друга. Тем не менее, мы тоже сочли это ликом и не использовали в решении.
На признаках друзей мы сгенерировали аналогичные признаки, что и на табличных данных основных пользователей. Следующий естественный шаг — отобрать топ признаков (мы взяли топ-100 по feature importance), а затем сделать агрегаты по топу признаков у друзей. Добавление таких усредненных по друзьям признаков увеличило качество еще на 1 пункт ROC AUC.
Ансамбль из коробки
После генерации признаков, добавления эмбеддинга по последовательности и агрегатов по друзьям, мы приступили к обучению модели. 
Хакатон длился меньше 2 суток, поэтому мы решили не тратить время на тюнинг гиперпараметров, а доверить это готовым фреймворкам. Выбрали LightAutoML, который часто используют в табличных соревнованиях на Kaggle. Из коробки LightAutoML выбивает хорошее качество, которое не так просто улучшить. Под капотом LightAutoML обучает самые популярные модели бустингов (CatBoost и LightGBM) и строит несколько уровней ансамблирования. 
К вечеру первого дня хакатона мы уже обучили первую модель. После этого наступило время улучшать бейзлайн, проверять идеи и привлекать более сложные модели, включая нейросетевые.
А где же граф?
Кейс хакатона назывался «Машинное обучение на графах», хотя в явном виде графов в этой задаче не было. 
Мы решили это исправить и самостоятельно восстановить ребра между пользователями. Нужно было:
перевести пользователей в некоторое пространство эмбеддингов;найти в этом пространстве k ближайших соседей к заданному пользователю;связать ребрами пользователя с его ближайшими соседями.
Соседей разумно искать среди пользователей обучающей выборки, по которым известны и признаки, и таргеты.
В качестве эмбеддинга пользователя вновь использовали TruncatedSVD разложение его признаков. В пространстве пониженной размерности искали ближайших соседей для пользователей обычной евклидовой метрикой. 
Нашли для каждого пользователя 4 ближайших соседей, применили аналог target encoding (для каждого соседа известен таргет, можно напрямую использовать таргет соседа, не боясь ликов и переобучения).Получили 4 дополнительных признака: таргет ближайшего соседа, таргет второго по удаленности соседа и т.д.Применили агрегаты на таргетах соседей (среднее, минимум, максимум).Кроме таргетов известны расстояния до соседей, их мы тоже использовали напрямую в качестве признаков, а также строили агрегаты на расстояниях.
Такой target encoding улучшил качество ещё на 1 пункт ROC AUC.
Время нейросетей
На сырых табличных данных нейросети обычно проигрывают бустингу. Другое дело – обработка последовательных данных (sequence processing) или графовых данных (graph processing). 
Основная идея графовых сетей — итеративное построение эмбеддингов вершин, где на каждой итерации учитывается информация с расширяющейся окрестности. 
В нашей задаче графовые данные оказались не совсем графовыми, у каждой вершины была только единичная окрестность. Однако, последовательные данные никак не пострадали от ограничений задачи. 
Для обучения эмбеддингов последовательности мы использовали библиотеку pytorch-lifestream. Эта библиотека не первый раз появляется в соревнованиях, например, в этом году ее использовали сразу несколько победителей и призеров в соревновании Data Fusion. С помощью pytorch-lifestream в self-supervised режиме построили эмбеддинги последовательностей (под капотом идея с маскированием, как в трансформерах). 
Финальная архитектура на рисунке.

Кто победил?
Особенность хакатона — необходимость защиты решения. 
Требовалось в 7-минутном формате запитчить модель. Здесь пригодился скилл грамотной подготовки слайдов и построения интересного устного рассказа. 
Организаторы определили шорт-лист из 5 команд по скору в лидерборде. Итоговые призовые места распределили жюри, опираясь на качество модели, оправданность и применимость архитектуры, а также проработанность устного доклада. В результате, наша команда заняла 1 место.

Эпилог
Что мы успели попробовать, но не зашло:
Графовые нейросети.Обучение нейросетевого энкодера на последовательностях end-to-end.Обучение независимых моделей на разных доменах данных.
Из очевидных вещей, которые хотелось попробовать, но не успели — заменить TF-IDF по последовательностям на полноценный word2vec. Обучить трансформер на таких объемах данных проблематично, а обычный word2vec, кажется, вполне реально. 
Если смотреть на feature importance финальной модели, то хорошо зашли расстояния и таргеты ближайших соседей, некоторые агрегаты по друзьям и признаки по последовательностям после SVD.

Что почитать?
Про фреймворк LightAutoMLКак я участвовал в соревновании по машинному обучению и занял второе место (и почему не первое)Как мы участвовали в чемпионате по DS длиной 3,5 месяцаКак и зачем мы начали искать бизнес-инсайты в отзывах клиентов с помощью машинного обученияНейросетевой подход к кредитному скорингу на данных кредитных историйЗнакомство с Apache Airflow: установка и запуск первого DAGа«Бесполезные» доклады о том, как кочегарить, инференсить и моделировать LTV: как прошёл Data Science Meet Up #2
Также подписывайтесь на Телеграм-канал Alfa Digital — там мы постим новости, опросы, видео с митапов, краткие выжимки из статей, иногда шутим.
"	хакатон, графы, VK, ВК, Альфа-Банк, машинное обучение, NLP, обработка текстов, нейронные сети, нейросети, соцсети, TF-IDF, SVD, LightAutoML, pytorch-lifestream, 1 место	1 место, хакатон, графы, альфа-банк, обработка текстов, svd, pytorch-lifestream, нейросети, tf-idf, lightautoml, nlp, машинное обучение, вк	13	1 место, хакатон, граф, альфа-банк, обработка текст, svd, pytorch-lifestream, нейросеть, tf-idf, lightautoml, nlp, машинный обучение, вк	1_NUM место_NOUN, хакатон_NOUN, граф_NOUN, альфа-банк_NOUN, обработка_NOUN текст_NOUN, svd_PROPN, pytorch-lifestream_PROPN, нейросеть_NOUN, tf-idf_PROPN, lightautoml_PROPN, nlp_NOUN, машинный_ADJ обучение_NOUN, вк_PROPN	хакатон_NOUN, граф_NOUN, альфа-банк_NOUN, обработка_NOUN текст_NOUN, svd_PROPN, pytorch-lifestream_PROPN, нейросеть_NOUN, tf-idf_PROPN, lightautoml_PROPN, nlp_NOUN, машинный_ADJ обучение_NOUN, вк_PROPN
3	https://habr.com/ru/company/amvera/blog/699058/	генерирование стихов, генерация текста, создание стихов нейронной сетью, rubert, генерация стихов нейронной сетью, amvera, amvera speech, amvera cloud	"Мне нравится концепция, согласно которой речь – это, в первую очередь, не способ коммуникации, а отражение сознания. В таком случае стихи - это отражение красоты сознания. Но сможет ли нейросеть сгенерировать стихотворения, похожие на рукотворные? Давайте попробуем сделать такой алгоритм.
Шаг 1 – выбираем архитектуру
Тренд последних лет в обработке естественных языков (NLP) - использование нейронных сетей. А если смотреть более узко, то - нейронных сетей архитектуры «трансформер», включающих блок внимания «attention». Суть подхода в том, чтобы использовать при кодировке как в энкодере эмбединга (вектор признаков на выходе слоя нейронной сети), так и в декодере, механизм «attention», позволяющий учитывать взаимосвязь между словами и «фокусировать внимание» нейронной сети только на контексте, имеющем значение для слова.
Одной из архитектур на основе трансформеров является ruBERT, его и возьмем. Но для чистоты эксперимента попробуем также использовать и более старый подход, а именно LSTM нейронную сеть.
Шаг 2 – собираем обучающую выборку
Для обучения, а вернее дообучения алгоритма используем: 
«Евгений Онегин» А.С. Пушкина. Произведение хорошо тем, что содержит большой объем «однотипного» в плане ритма и структуры текста.Сборник произведений М.Ю. Лермонтова.И немного других стихов других авторов.
Шаг 3 – пробуем использовать обычную LSTM на всех трех наборах данных
Получаем следующий результат:
онегин тучки страстей вот, нет, и людей, я тени не деревне начинает брызги вас, его им дамских обесславить виясь, искуситель приведёт вражду, воскресить ведает полевой охотно своенравной нетерпеливо и правда ли, придает улыбкой конце бы провождая, всё забрезжит нас евгений везде звучные изнывать, с клонит смиренной природа
Отдельные слова похожи на оригинал – смысла нет! Сделаем еще попытку.
Шаг 4 – Оставим только «Евгения Онегина» для обучения
Применим параметр temperature = 0,3
«мой дядя самых честных правил, когда не в шутку занемог, он уважать себя заставил и лучше выдумать не мог. а нынче бы в обществе принесть что ольга резвая твоя?» бывало, льстивый ты, ни карт, ни зарею плод. и, сохраненная судьбой, и холод гордости спокойной, и, флер и кинжал, парисокружных городков, подходит в таинственной гондоле; с ней обретут и мнения супруга и вслед и захотел. , кипящим в действии пустом. когда б в сраженьях изувечен, гляди… как грустно, с закон несчастной вменяя и неотвязчивый лорнет привычкой герою. , и теплотойи начинает понемногу родной и прыгать не хотела качая после, на земле взять притворным в поле вертится около молчанье,и в легковерным ослепленьем чего, сплин в сениюделе скончался бабушкам и дедам не потому ль, что в высшем свете теперь являться за цели, как будто громом поражен. и, умиленными очами неверный зацепляя лед,людская молвь и конский топ но я бороться вручаю эпиграмм.
Стало лучше – появилось что-то похожее на рифму, хотя смысла по прежнему нет. 
Используем temperature = 1.2
«мой дядя самых честных правил ,когда не в шутку занемог ,он уважать себя заставили лучше выдумать не мог.а нынче бы в обществе принестьеще соблазнительную честь?а счастье , которым и можетне потому ль , что такое:в субботу. неба безмятежных ,ничтожный призрак , живой ,скользя , ползет –и как-нибудь ,и в шуме и любой многих ,как сади себя пиит.и , зарыдав , княжну алину,неумолимых , неподкупных ,непостижимых условною красойс нем , по таинственному древу;запретный плод сиживал один. 
Смысла тут по прежнему нет. Хотя, если сильно постараться и представить, что это «очень современное искусство», возможно, даже получится его найти!
А если добавить рока?
Попробуем AC/DC с обучением на их песнях, а не на «Евгении Онегине», разумеется.

Неплохо, неплохо, но без музыки непонятно.
Шаг 5 – применим тяжелую артиллерию ruBERT
Используем Temperature = 0,3
•И шагом едет в чистом поле, •В мечтанья погрузясь;  •Душа его полна сожалений: ""Что-то с •Ольгой стало? –•В ней сердце долго ли страдало и как будто бы похорошело. •Но вот уж луна обходит кругом небесный свод... •И всё ей кажется темным да мрачным!.. •Она глядит не на дорогу –видит лишь верхушки лип…•Да полно!•Всякое может быть!"" •Тут он вспомнил про •Ленского своего соседа по имени •Владимир•Петушков.•Тот недавно овдовел совсем (жена умерла вскоре после родов), а детей нет у него вовсе.""
Ого-го, рифма белая, конечно, но уже похоже на творчество человека!
Добавим температуры. Temperature = 0,6
И шагом едет в чистом поле, •В мечтанья погрузясь;•Душа его полна сожалений: ""Где же ты? •Уж эти мне поэты!"" •И вот он подъехал к селению. •Открылись ему двери... и входит туда грустный •Евгений… нет ее! •Он проникает через другие покои (замечу мимоходом) And find shimself in a room full of women sitting alone aga inst fire tablets by whichroast beef is spread ingon the irthroat’es while still dancing men a round them a replay ingcards at the door way opposite him. •He looked for help but found noone else than his own assistant swhow •So he went back into town hope lessly hungry that night without saying any thing except what happenste thow was wrong so far"".
Так, началось за здравие, а закончилось за «London is the capital of Great Britain».
Хорошо, добавим еще немного температуры.
Temperature = 2.1
•«Мой дядя самых честных правил,•Когда не в шутку занемогнув(Что случалось с ним довольно часто),он уважать себя заставил;Но если бы даже очень захотел...», —сказал е я отец. «Не может быть! -воскликнул •Евгений.-•Я это видал». •И вот проказник вырос гордый и надменный: он уверен уж порядком отомстить обидчику*. •Но выслушав печальный рассказ отца о том самом случае(Как узнал потом наш герой-сыщик)сжалился родитель молодой и пожалел его:-•Мой бедный мальчик!-промолвил наконец Пророк слезным голосом амнистии невинной полный освободить тебя обещал. Ах как боялся я утраты за друга жизни драгоценной!..Вдруг мысль яснеет мне моя:""еслиб только мог оказать услугу я этому простому человеку!"" •Тут мой •Онегин вспомнил про обещание своё Остаться наедине со мной Дабы рассеять горе напрасное опять задумчивой души его младое воображенье Раскаялось.
Тут без комментариев – делайте выводы сами.
Попробуем начать с фразы «Татьяна вы больны …»
Татьяна вы больны,•Вы слишком чувствительны для других; Досугам посвятитесь беспечным:•Сочтете страстной жизнь чужую –•И вас тоска кольнет. •Когда ж и где?•Ужели в ссылке глухой?.. •Нет-с!•Право же (совесть во мне говорит), я счастлив… хоть не без греха».•Тут он зевнул сладко такою складкою губок своих длинных and went to the table where •Annette lay alone against a window closed by fire.•Там на столе горела свеча...И рядом с ней лежал листок бумаги сложенный тобой.•Together they wrote something together in such simple language that•I could easily under stand without muchef for to f mind what was written there in «Вот это да!» –сказал Евгений усмехаясь
Действительно – вот это да, London is the capital of Great Britain!
Хорошо, попробуем начать с «Когда я на почте служил ямщиком …»
Я три раза был ранен, но не дрогнул.•Я тридцать лет холостой: ни женушки ласки нетолерантной, ни подруг нежных зимних вечеров.Пора мне сделаться умней и тверже -жениться пора! •Но полно.-•Мне скоро сорок девять годков.—сказал как-то •Евгений*. •И добавил с какой то непонятной улыбкой •(Что за ерунда?)И точно также замолчав запрягнул свою лошадку...Опять задумался он над своей судьбой.""Нучто ж? •Судьбу мою поправлю непременно!-решил наконец •Владимир•Ленской****** ""Но где же взять денег?"" —снова обратился к нему поэт со слезным вопросом"". «Найдешь»,—отвечал ему приятельски осклабя взор "".Слушай!"" –воскликнул •Онегин протягивая руку юноше ""Даты садись поскорей да разгружайся"",–торопил его сосед по саням """"Как здоровье твое?"", −спросил
А если начать с «я свободен словно …»
Я свободен словно ветер,•Я могу ль тебя любить? •Мой милый! что ты подумал?..       Ты спросил меня однажды: «Кого?»•Я сказал ему. •Он отвечал мне также».*****«Кто же он?», –снова раздался немой вопрос.-""Никто; но кто ж его мать?». •И вот я здесь сам не знаю…-начал было •Онегин рассуждать вслух своим замечательным слогом о разных материях и наконец кончил:-•Мой бедный •Ленский!.. ужели для того лишь создал бог наш райские кущи, чтобы над ними властвовать нам?-•Нет (совесть в том порукой), однако мысль благая у него иногда шевелится).–•Ну так сделай одолженье твое,-обратился•Евгений к другу с нежданной просьбой своей души. И друг тотчас согласился...
И напоследок, начинаем с «у меня во дворе ходит девочка с каре …»
А у меня во дворе ходит девочка с каретом,Ей тринадцать лет; она грустна и молчалива. Когда ж настанет время ей стать девицей?Когда падучая звезда упадет к ней в окно,-ей будет грустно! Но не беда: вскоре отрада придет другая:-улыбнись ты сам себе!-и тоска по деве молодой пройдет.Тогда запиши ее при луне под березой...И скоро твой печальный глас раздается опять -звучно так же ясно как день!..Но эта песня любви тебе чуждою кажется.""-Какая глупая песенка!"" подумал Евгений*.
Итого 
Что-то похожее на смысл у нас получилось с использованием ruBERT при аккуратном подборе параметра “temperature”.
Выводы делать вам и только вам. Но давайте будем честны: нейросеть, конечно, не Пушкин, но в отличие от стихов некоторых людей, в нейростихах можно даже найти смысл, если очень долго искать;)
Статья написана на основе работы Максима Макарова @maxcoolmakarov.
Ознакомиться с кодом, примерами, ML моделями и т.д. вы можете по ссылке.
А если вы хотите где-то развернуть это (или собственное) решение, то вы можете сделать это бесплатно в нашем облаке – Amvera. Мы проводим бета-тест, и будем рады бесплатно предоставить вам вычислительный ресурс в обмен на обратную связь. 
"	обработка естественного языка, NLP, нейронные сети, трансформер, LSTM, ruBERT, Евгений Онегин, Лермонтов, AC/DC, temperature, сгенерировать стихотворения, стихи	rubert, сгенерировать стихотворения, lstm, нейронные сети, temperature, евгений онегин, ac/dc, стихи, обработка естественного языка, лермонтов, nlp, трансформер	12	rubert, сгенерировать стихотворение, lstm, нейронный сеть, temperature, евгений онегин, ac/dc, стих, обработка естественный язык, лермонтов, nlp, трансформер	rubert_PROPN, сгенерировать_VERB стихотворение_NOUN, lstm_PROPN, нейронный_ADJ сеть_NOUN, temperature_NOUN, евгений_PROPN онегин_PROPN, ac/dc_PROPN, стих_NOUN, обработка_NOUN естественный_ADJ язык_NOUN, лермонтов_PROPN, nlp_NOUN, трансформер_NOUN	rubert_PROPN, lstm_PROPN, нейронный_ADJ сеть_NOUN, temperature_NOUN, ac/dc_PROPN, стих_NOUN, лермонтов_PROPN, nlp_NOUN, трансформер_NOUN
4	https://habr.com/ru/company/unidata/blog/698268/	entity resolution, data matching, record linkage, data deduplication, fuzzy matching, inexact matching, неточные дубликаты, неточное сопоставление, entity matching	"Всем привет!
Мы хотим рассказать немного об entity resolution как об академической дисциплине, о доступных инструментах для решения этой задачи, и о нашем опыте с одним из инструментов.

Смысл термина
Дедупликацией (deduplication) называется задача по поиску повторов в любых данных. В нашей статье мы будем рассматривать такой процесс только для структурированных данных — то есть для таблиц, хотя инструменты для дедупликации разрабатываются также и для слабоструктурированных моделей данных, таких как, например XML и JSON.
В нашем контексте ""повторы"" называются дубликатами (duplicates) – это структурированные записи, описывающие одну и ту же сущность (entity). Традиционно, data matching — это дедупликация одной таблицы, тогда как поиск дубликатов по нескольким таблицам называется связыванием записей (record linkage). Эта задача тесно связана с data matching'ом по очевидным причинам.
Такие дубликаты могут быть точными (exact) или неточными (fuzzy, inexact). Поскольку степень неточности на самом деле не ограничена, для поиска нечётких дубликатов, особенно в больших данных, требуются различные сложные алгоритмы и наборы инструментов. О том, какие методы могут применяться для этого, речь пойдет в следующей части.
Наконец, эти задачи объединяются под термином entity resolution (букв. разрешение сущностей, ER). На самом деле, все введённые термины более-менее взаимозаменяемы, но это устоявшееся формальное разделение.
Картинка из [1], которая показывает, сколько терминов применяется в области для примерно одного и того же.
В основном, инструменты ER фокусируются на данных, принадлежащих к областям, полезным для бизнеса. Не было бы преувеличением сказать, что многие компании и организации пользуются табличными базами данных.
Такие базы данных либо изначально заполняются вручную, либо наполняются с помощью интеграции данных из различных внешних и внутренних источников. И зачастую данные в них страдают от различных проблем качества, таких как неполнота, неконсистентность (конфликты в данных), и наконец, ошибки и опечатки.
Всё это ведёт к потере прибыли (в среднем компания может потерять 12% выручки) и другим убыткам, например, репутационным (The cost of bad data: stats). Таким образом, entity resolution — это задача по улучшению качества данных (data quality) и, обобщая, задача интеграции данных (data integration).
Как и чем можно искать дубликаты?
Эта область в академическом сообществе существует очень давно (первые статьи по этой теме относятся к шестидесятым годам) и в настоящее время пользуется огромным вниманием, особенно в связи с расцветом классического машинного и глубокого обучения.
Существует значительное количество обзоров, например, [1-5], а также книга [6]: более старые из приведенных материалов содержат алгоритмы, которые до сих пор применяются в области и ставшие “классическими”. Также entity matching с использованием deep learning был посвящён keynote-доклад на конференции SIGMOD 2021 [7] — флагманской конференции сообщества баз данных. 
Наконец, есть leaderboard на сайте paperswithcode, где соревнуются state-of-the-art (SOTA; в академическом сообществе обозначает наилучший результат) решения; стоит заметить, что там находятся только решения, основанные на глубоком обучении.
Система для entity resolution — это система, которая из набора сущностей составляет набор уникальных сущностей. С технической точки зрения, она обычно классифицирует пары записей как пары дубликатов (matches) или не-дубликатов (non-matches). Рассмотрим абстрактное устройство такой системы по материалам [1]:

Indexing (Blocking) — шаг, главной задачей является быстрое отбрасывание как можно большего количества ненужных сравнений пар без потери дубликатов. На нём похожие записи помещаются в блоки на основании некоторого критерия так, чтобы на следующем этапе попарно сравнивать только записи, попавшие в один блок. На вход алгоритм, реализующий данный шаг, получает сам датасет, а на выходе — набор блоков.Block Processing — шаг, на котором блоки дополнительно очищаются от шума. Набор блоков, полученный на предыдущем шаге, трансформируется в новый набор блоков с примерно таким же показателем recall, но с более высоким precision. Как вариант, можно перебрасывать записи между блоками.Matching — шаг, который оценивает похожесть отобранных записей. Чаще всего это некоторая функция похожести, которая может быть основана, например, на редакционном расстоянии. Его выход — это взвешенный граф похожести, где узлы — это записи, а рёбра содержат оценку их похожести. При этом граф не связный: оценки похожести получаются только для пар, которые находятся в одном блоке.Entity Clustering — шаг, который собирает похожие пары в кластеры; таким образом создаётся набор уникальных сущностей. 
В основном как классические, так и SOTA-решения опираются на этот концепт. Обзоры, которые мы приводим выше, предлагают обширные списки алгоритмов для каждого шага. Авторами [1, 2] были выделены различные признаки для каждого шага, по которым можно классифицировать алгоритмы, его реализующие. 
Одним из важных признаков является деление алгоритмов блокинга, матчинга и кластеризации на schema-aware и schema-agnostic. Первая группа работает только с теми атрибутами, которые указаны в конфигурации системы (например, экспертом по домену), и поэтому такие методы используются только для реляционных данных. 
Вторая же группа рассматривает все существующие атрибуты, и поэтому может применяться для более слабоструктурированных данных, таких как XML-базы или базы знаний. Об остальных признаках, таких как, например, использование и не-использование обучения, о видах матчинга и т.п. можно подробнее прочитать в указанных статьях.
Говоря более подробно именно о глубоком обучении и нейросетях в данной области, стоит сказать, что методы, использующие их, не являются полноценными системами для ER. В обзоре [3] собран обширный список методов,  актуальный на 2021 год. Они соотнесены с шагами entity resolution, изложенными выше. 
Только два метода из 21 рассмотренных решают задачу блокинга, а все остальные нацелены на иные шаги. Одним из ярких отличий таких методов является специальная предобработка данных с получением эмбеддингов — т.е., векторных репрезентаций. Половина из рассмотренных методов её использует.
Если же говорить о архитектурах нейросетей,которые используются для матчинга, а не только для получения репрезентаций, то они делятся на две группы — RNN-подобные и Transformer-подобные. Мы уже приводили keynote-доклад [7], посвящённый глубокому обучению в области в целом и модели Ditto [8] в частности. 
В данной системе используется дообучение (fine-tuning) BERT на задаче, сформулированной как классификация пары последовательностей. В этом месте можно отметить, что задача entity resolution, традиционно относившаяся к области баз данных, теперь начинает совмещаться с современными разработками в обработке естественного языка (NLP). 
Поскольку нас интересовала не только научная, но и рабочая сторона вопроса, мы рассматривали инструменты, подпадающие под следующие критерии применимости:
Открытый исходный кодНаличие некоторой степени ""завершённости"":Возможность без особых усилий запустить инструмент на современной машине (Win10 или любой современный дистрибутив Linux) — т.е., код поддерживался в работоспособном состоянииEnd-to-end решение, т.е., выполняет полный процесс, а не только один из его шагов: в идеале для работы решения нужно иметь только сами данные и человека, который обучен работе с системойНе основаны на глубоком обучении, т.к. получение обучающих данных нужного размера в индустриальном сценарии требует значительных дополнительных усилий и ресурсов Наличие возможности работать с одной таблицей, т.е. именно с дедупликацией, а не только  со связыванием
В таблице 1 приведён список инструментов, которые мы рассматривали:
ИнструментЛицензияСтекКомментарииZingg AGPL-3.0Java, Spark, Docker“Цельное” решение для дедупликации и связывания записей. О нём будет рассказано дальше.Dedupe + csvdedupeMITPythonDedupe — библиотека, на основе которой можно разрабатывать инструменты, подобные Zingg. Соответственно, csvdedupe — простой пример CLI-инструмента, разработанного на основе Dedupe. Разработчики приводят примеры кода в документации, но пользоваться Dedupe просто так не получается, т.к. в памяти он может обработать менее 10K записей. В противном случае начинаются проблемы с производительностью. Предпочтительным способом работы с dedupe на больших данных является использование её поверх SQL СУБД.RecordLinkageBSD-3-ClausePythonБолее низкоуровневый пакет, чем Dedupe. Предоставляет доступ непосредственно к алгоритмам — т.е., пайплайн поиска дубликатов разработчик определяет сам. В документации указано, что он предназначен для исследований на небольших объёмах данных.JedAIApache-2.0Java, DockerGUI-приложение. По словам разработчиков, предоставляет три подхода (workflow) к дедупликации. Внутри этих подходов на каждом шаге доступно большое количество вариантов алгоритмов для него. На деле удалось поработать только с одним, основанном на Similarity Join, и из него не удалось получить никакого адекватного результата. В дополнение к этому, приложение само по себе не очень адекватно — выдаёт необъяснимые ошибки, теряет результаты работы. Авторы не предоставляют примеры того, в каком именно формате должны быть CSV-данные для того, чтобы хотя бы запустить их алгоритмы на них.SplinkMITPython, SparkБиблиотека, похожая на RecordLinkage, но ещё более низкоуровневая - правила, по которым записи будут сравниваться, пишутся руками (т.е., правила вида ""l.first_name = r.first_name""). Требует SQL-бэкенда.
Исходя из наших потребностей, мы остановились на Zingg, как на решении, отвечающим большинству критериев из нашего запроса.
Что такое Zingg? Как им пользоваться?
Zingg — это решение для дедупликации, разработанное ZinggAI. Разработчики позиционируют его как индустриальный инструмент в ELT-пайплайне, чьё главное назначение — быстро и масштабируемо построить единый источник достверных данных для важнейших бизнес-объектов либо после Extraction, либо после Load для подготовки данных к анализу.
Zingg написан полностью на Java на основе Apache Spark; может устанавливаться как Docker-образ или разворачиваться на Spark-кластерах. Дополнительно есть обёртка для Python, которая позволяет пользоваться функциональностью Zingg как импортируемым модулем. Разработчики Zingg заявляют следующее:
Zingg может производить как связывание, так и дедупликациюСправляется с любыми сущностями, такими как клиенты, пациенты, товары и т.д.Работает с большим количеством видов источников данных и входных форматов файловМасштабируется на большие объёмы данныхОснован на машинном обучении и устроен согласно описанной выше модели:Модель для блокинга в Zingg — это алгоритм, с помощью которого подбираются blocking functions, наиболее хорошо сохраняющие похожие записи в одном блоке. Здесь можно найти пример работы таких функций от разработчиков. Также Zingg позволяет определять и добавлять такие функции вручную в том случае, когда это требуется для домена знаний.Для того, чтобы сравнить похожесть записей, Zingg вычисляет множество признаков для каждого поля, в основном связанные со сравнением строк (по длине, редакционному расстоянию и т.д.), получая вектор похожести (similarity vector) для каждой сравниваемой пары. После чего Zingg использует логистическую регрессию для того, чтобы оценить, является ли проверяемая пара записей дубликатами. Обычно в таких системах коэффициент 0.5 принимается за пороговое значение, но разработчики ZIngg заявляют, что они его оптимизируют под каждый датасет — т.е., теоретически оно может отличаться от 0.5.Использует активное обучение и предлагает для него интерактивный интерфейс. Активное обучение — это такой вид машинного обучения, который проводит предварительный отбор небольшого количества неразмеченных данных из общей выборки и предлагает пользователю их разметить. Разработчики Zingg заявляют, что пользователю достаточно найти 30-40 пар настоящих дубликатов для того, чтобы на выходе получить адекватную модель.
Процесс установки Zingg соответствует заявленному: Docker-образ устанавливается без проблем в Ubuntu. Графического интерфейса у Zingg нет, но есть CLI. Процесс работы с Zingg выглядит следующим образом.
Сначала под входной набор данных нужно написать JSON-конфиг, описывающий формат входного файла, его схему, типы полей, вид матчинга для каждого поля, формат выходных данных, и наконец, указать два параметра labelDataSampleSize и numPartitions. Первый параметр — это тот процент записей, в которых будут искаться пары для разметки, и разработчики советуют использовать значения в диапазоне от 0.0001 до 0.1. Второй параметр — это количество партиций Spark, на которых будет параллелизовываться процесс. Здесь разработчики советуют использовать количество ядер, умноженное на число от 20 до 30.  Пример конфиг-файла можно увидеть вот здесь.
Дальнейший процесс состоит из фаз, определённых в самом Zingg. 
Первая фаза — это findTrainingData, запускающая алгоритм поиска пар записей, которые могут быть предложены для разметки пользователю. Эту и следующую фазы необходимо запускать несколько раз, поскольку, как сказано выше, для “адекватности” будущей модели требуется по крайней мере 30-40 положительных примеров дубликатов.
./scripts/zingg.sh --phase findTrainingData --conf path/to/conf.json
Следующая фаза — label, интерактивная разметка найденных на предыдущем шаге пар записей.
./scripts/zingg.sh --phase label --conf path/to/conf.json
Выглядит она следующим образом:

После нахождения достаточного количества настоящих дубликатов, можно запускать фазу train — собственно, обучение модели.
./scripts/zingg.sh --phase train --conf path/to/conf.json
Наконец, для дедупликации одного датасета нужно запустить фазу match, а для связывания записей в двух датасетах — фазу link.
./scripts/zingg.sh --phase match --conf path/to/conf.json
Примечание: для удобства пользователя первая и вторая, а также третья и четвёртая фаза могут быть объединены в пары, и запускаться командами findAndLabel и trainMatch соответственно.
Выходные данные Zingg для одного датасета
Тем записям, которые Zingg распознал как дубликаты, присваивается один и тот же номер в колонке z_cluster. Колонки z_minScore и z_maxScore содержат минимальную и максимальную оценку похожести данной записи на записи в этом же кластере.
Тестирование и бенчмарк Zingg
В сообществе ER широко доступны датасеты для бенчмаркинга. Подборка Benchmark datasets for entity resolution | Database Group Leipzig содержит самые известные из них, но есть также и другие, например, WDC Product Data Corpus или Magellan Data Repository. 
Поскольку задача entity resolution может быть рассмотрена как задача бинарной классификации, то оцениваются решения для дедупликации чаще всего с помощью Precision, Recall, и F1, хотя существуют и более необычные, о которых можно прочитать, например, в статьях [10, 12]. 
Оценка производительности обычно производится замером времени “по часам” (wall-clock running time). В нашем случае мы не проводили точных замеров времени, но ручной поиск 30-40 пар требуемых дубликатов с интерактивной разметкой в каждом случае занимал около получаса, а сам процесс обучения модели на размеченных данных (т.е. работа фазы train) занимал не более 10 минут.
Разработчики Zingg предоставляют несколько моделей, пред-обученных на известных бенчмарк-датасетах, но не оценивают их результаты. Поэтому мы решили проводить процесс обучения моделей с нуля. Для бенчмарка Zingg мы выбрали платформу Frost (Snowman) [10], как наиболее современную и доступную. Существуют также и другие платформы, такие как FEVER [11] и EMBench++ но они были разработаны в конце нулевых, и на сегодняшний день не поддерживаются.
Все датасеты, кроме первого, доступны публично: помеченные двумя звёздочками принадлежат группе Database Group Leipzig, а остальные доступны на самой платформе Snowman.
Мы проводили бенчмарк в следующих условиях:
Zingg 0.3.4, Docker Engine v20.10.8Ubuntu 20.04 для WSL2Windows 10 Pro, 21H1, сборка 19043.1645Intel(R) Core(TM) i7-10510U CPU16GB RAM (12GB выделено под виртуализацию WSL2)Формат входного и выходного файлов - CSVПараметр numPartitions был установлен равным 1, параметр labelDataSampleSize указан в таблице
Таблица 2: Результаты нашего бенчмарка
*При переносе на Хабр таблица оказалась не очень читаемой. Просим прощения, уперлись в ограничения верстки
ДатасетДоменПоляКол-во записейКол-во размеченных парlabelDataSampleSizePrecRecF1AccИмена собственные с опечатками*Физические лицаfirst, last, patro1000640.51.00.770.871.00Magellan Songs 1MПесниartist_familiarity,artist_hotness,artist_name,duration,id,release,title,year10000001800.0050.40.970.571.00Geographical Settlements**Поселения: координаты и названияlat,ontology,lon,label,id,type,ele,typeDetail30541560.50.430.70.530.99AltoSightТовары: Флешкиbrand,instance_id,name,price,size8351130.50.380.780.510.98FreeDB CDs (очищенный от записей со сбоем кодировки)Описания дисков с музыкойartist,category,genre,id,title,tracks,year75271070.50.160.780.261.00SIGMOD Notebook LargeТовары: Ноутбукиbrand,cpu_brand,cpu_frequency,cpu_model,cpu_type,dimensions,hdd_capacity,instance_id,ram_capacity,ram_frequency,ram_type,ssd_capacity,title,weight3371420.50.310.310.310.96Affiliations**Названия научных учрежденийid,affil22601210.50.380.220.280.99
*Данные, которые мы сгенерировали для этого датасета, были очень примитивными, и выглядели примерно так:

Как можно легко увидеть из таблицы, практически все эксперименты показывают довольно высокий recall и низкий precision; это означает, что Zingg выдаёт большое количество ложноположительных ответов, но достаточно низкое ложноотрицательных — т.е., находит большой процент настоящих дубликатов, но его выдача засорена ненастоящими. Для нашего гипотетического индустриального сценария данный результат является вполне удовлетворительным. 
Низкие показатели в эксперименте Affiliations и Notebook Large связаны с тем, что Zingg не использует технологии, которые могут оперировать со “смыслом” данных (например, эмбеддинги), а использует только метрики похожести строк. Например, первая пара из датасета Affiliations — это ложноположительный ответ Zingg, а вторая — ложноотрицательный, хотя редакционное расстояние меньше между записями первой пары.


Такая же примерно ситуация произошла и с датасетом Notebook Large, поскольку он содержит объявления о продаже товаров, автоматически собранные с Amazon и подобных сайтов.
Колонка Accuracy в таблицу включена, поскольку авторы Zingg, несмотря на то, что предлагают несколько предобученных моделей на бенчмарк-датасетах, не указывают результаты, ими полученные. 
Единственная метрика, о которой на данный момент они говорят — это Accuracy, но, как видно из таблицы, и как интуитивно понятно по постановке задачи, эта метрика при более-менее любых результатах должна быть близка к единице, так как дубликатов не может быть много относительно общего числа записей. Стоит отметить, что у разработчиков в планах есть добавление подсчёта Precision, Recall и F1, но пока эта функциональность не была реализована даже экспериментально.
Как можно заключить из представленных экспериментов, Zingg — вполне подходящее решение для достаточно простых случаев. Тогда как для более сложных сценариев (например, автоматически собранные данные из различных источников, а не введённые вручную данные физических лиц) может потребоваться использование более наукоёмких решений. Большим преимуществом Zingg является “бесшовная” поддержка большого количества форматов данных и удобство использования, что в индустриальном сценарии может стать ключевым фактором.
Литература
Christophides, V., Efthymiou, V., Palpanas, T., Papadakis, G., & Stefanidis, K. (2020). An overview of end-to-end entity resolution for big data. ACM Computing Surveys (CSUR), 53(6), 1-42.Papadakis, G., Skoutas, D., Thanos, E., & Palpanas, T. (2020). Blocking and filtering techniques for entity resolution: A survey. ACM Computing Surveys (CSUR), 53(2), 1-42.Barlaug, N., & Gulla, J. A. (2021). Neural networks for entity matching: A survey. ACM Transactions on Knowledge Discovery from Data (TKDD), 15(3), 1-37.Elmagarmid, A. K., Ipeirotis, P. G., & Verykios, V. S. (2006). Duplicate record detection: A survey. IEEE Transactions on knowledge and data engineering, 19(1), 1-16.Brizan, D. G., & Tansel, A. U. (2006). A. survey of entity resolution and record linkage methodologies. Communications of the IIMA, 6(3), 5.Peter Christen. 2012. Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection. Springer Science & Business Media.Tan, W. C. (2021, June). Deep Data Integration. In SIGMOD Conference (p. 2).Yuliang Li, Jinfeng Li, Yoshihiko Suhara, Jin Wang, Wataru Hirota, and Wang-Chiew Tan. 2021. Deep Entity Matching: Challenges and Opportunities. J. Data and Information Quality 13, 1, Article 1 (March 2021), 17 pages. https://doi.org/10.1145/3431816Köpcke, H., Thor, A., & Rahm, E. (2010). Evaluation of entity resolution approaches on real-world match problems. Proceedings of the VLDB Endowment, 3(1-2), 484-493.Graf, M., Laskowski, L., Papsdorf, F., Sold, F., Gremmelspacher, R., Naumann, F., & Panse, F. (2022). Frost: a platform for benchmarking and exploring data matching results. Proceedings of the VLDB Endowment, 15(12), 3292-3305Köpcke, H., Thor, A., & Rahm, E. (2009). Comparative evaluation of entity resolution approaches with fever. Proceedings of the VLDB Endowment, 2(2), 1574-1577.Panse, F., & Naumann, F. (2021, April). Evaluation of duplicate detection algorithms: from quality measures to test data generation. In 2021 IEEE 37th International Conference On Data Engineering (ICDE) (pp. 2373-2376). IEEE.
Статью подготовили: Анна Смирнова и Георгий Чернышев"	entity resolution, дедупликация, повторы, дубликаты, улучшение качества данных, интеграция данных, data quality, data integration, эмбеддинги, векторные репрезентации, нейросети, открытый исходный код, Zingg	дубликаты, нейросети, entity resolution, data matching, data quality, векторные репрезентации, zingg, интеграция данных, data integration, повторы, открытый исходный код, дедупликация, улучшение качества данных, entity matching, record linkage, эмбеддинги	16	"дубликат, нейросеть, entity resolution, data matching, data quality, векторный репрезентация, zingg, интеграция данные, data integration, повтор, открытый исходный код, дедупликация, улучшение качество данные, entity matching, record linkage, эмбеддинг
"	"дубликат_NOUN, нейросеть_NOUN, entity_NOUN resolution_NOUN, data_NOUN matching_NOUN, data_NOUN quality_NOUN, векторный_ADJ репрезентация_NOUN, zingg_PROPN, интеграция_NOUN данные_NOUN, data_NOUN integration_NOUN, повтор_NOUN, открытый_ADJ исходный_ADJ код_NOUN, дедупликация_NOUN, улучшение_NOUN качество_NOUN данные_NOUN, entity_NOUN matching_NOUN, record_NOUN linkage_NOUN, эмбеддинг_NOUN
"	"дубликат_NOUN, нейросеть_NOUN, entity_NOUN resolution_NOUN, data_NOUN matching_NOUN, data_NOUN quality_NOUN, векторный_ADJ репрезентация_NOUN, zingg_PROPN, интеграция_NOUN данные_NOUN, data_NOUN integration_NOUN, повтор_NOUN, дедупликация_NOUN, entity_NOUN matching_NOUN, record_NOUN linkage_NOUN, эмбеддинг_NOUN
"
